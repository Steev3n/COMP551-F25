{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U9V9UODBoVPh",
        "outputId": "e0212db5-09d7-4288-e596-152451dc370e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train batches: 375\n",
            "Validation batches: 94\n",
            "Test batches: 79\n",
            "Computed mean: 0.2860, std: 0.3530\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "Python(35540) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
            "Python(35541) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
            "Python(35542) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
            "Python(35543) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch image tensor shape: torch.Size([128, 1, 28, 28])\n",
            "Batch labels tensor shape: torch.Size([128])\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision import datasets, transforms\n",
        "import numpy as np\n",
        "\n",
        "def compute_fashion_mnist_mean_std(root=\"./data\"):\n",
        "    \"\"\"\n",
        "    Load raw FashionMNIST training data, convert to float in [0,1],\n",
        "    and compute global mean and std over all pixels, as recommended in CS231n:\n",
        "    center data to mean 0 and normalize its scale.\n",
        "    \"\"\"\n",
        "    # Load once without transforms to access raw uint8 data\n",
        "    raw_train = datasets.FashionMNIST(\n",
        "        root=root,\n",
        "        train=True,\n",
        "        download=True,\n",
        "        transform=None\n",
        "    )\n",
        "\n",
        "    # raw_train.data: shape [60000, 28, 28], dtype uint8 in [0, 255]\n",
        "    train_data = raw_train.data.float() / 255.0 # match ToTensor scaling\n",
        "\n",
        "    mean = train_data.mean().item()\n",
        "    std = train_data.std().item()\n",
        "    return mean, std\n",
        "\n",
        "\n",
        "def get_fashion_mnist_datasets(root=\"./data\", val_ratio=0.2, seed=551):\n",
        "    \"\"\"\n",
        "    Acquire FashionMNIST, compute normalization statistics on training set,\n",
        "    and return normalized train, validation, and test datasets.\n",
        "\n",
        "    - Uses the default 28x28 version.\n",
        "    - Uses the 60k official training split for train + validation.\n",
        "    - Uses the 10k official test split as test.\n",
        "    \"\"\"\n",
        "    mean, std = compute_fashion_mnist_mean_std(root)\n",
        "\n",
        "    train_transform = transforms.Compose([\n",
        "        transforms.ToTensor(), # [0, 255] -> [0, 1]\n",
        "        transforms.Normalize((mean,), (std,)) # zero mean, unit-ish variance\n",
        "    ])\n",
        "\n",
        "    test_transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((mean,), (std,))\n",
        "    ])\n",
        "\n",
        "    full_train_dataset = datasets.FashionMNIST(\n",
        "        root=root,\n",
        "        train=True,\n",
        "        download=True,\n",
        "        transform=train_transform\n",
        "    )\n",
        "\n",
        "    test_dataset = datasets.FashionMNIST(\n",
        "        root=root,\n",
        "        train=False,\n",
        "        download=True,\n",
        "        transform=test_transform\n",
        "    )\n",
        "\n",
        "    # Split 60k training samples into train and validation\n",
        "    total_train = len(full_train_dataset) # should be 60000\n",
        "    val_size = int(val_ratio * total_train)\n",
        "    train_size = total_train - val_size\n",
        "\n",
        "    generator = torch.Generator().manual_seed(seed)\n",
        "    train_dataset, val_dataset = random_split(\n",
        "        full_train_dataset,\n",
        "        [train_size, val_size],\n",
        "        generator=generator\n",
        "    )\n",
        "\n",
        "    return train_dataset, val_dataset, test_dataset, mean, std\n",
        "\n",
        "\n",
        "def get_fashion_mnist_loaders(\n",
        "    root=\"./data\",\n",
        "    val_ratio=0.2,\n",
        "    batch_size=128,\n",
        "    num_workers=2,\n",
        "    seed=551\n",
        "):\n",
        "    \"\"\"\n",
        "    Convenience function that wraps dataset acquisition and returns\n",
        "    DataLoaders for train, validation, and test sets.\n",
        "    \"\"\"\n",
        "    train_dataset, val_dataset, test_dataset, mean, std = get_fashion_mnist_datasets(\n",
        "        root=root,\n",
        "        val_ratio=val_ratio,\n",
        "        seed=seed\n",
        "    )\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    return train_loader, val_loader, test_loader, mean, std\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train_loader, val_loader, test_loader, mean, std = get_fashion_mnist_loaders()\n",
        "\n",
        "    print(f\"Train batches: {len(train_loader)}\")\n",
        "    print(f\"Validation batches: {len(val_loader)}\")\n",
        "    print(f\"Test batches: {len(test_loader)}\")\n",
        "    print(f\"Computed mean: {mean:.4f}, std: {std:.4f}\")\n",
        "\n",
        "    # Inspect one batch shape\n",
        "    images, labels = next(iter(train_loader))\n",
        "    # images shape: [batch_size, 1, 28, 28]\n",
        "    print(f\"Batch image tensor shape: {images.shape}\")\n",
        "    print(f\"Batch labels tensor shape: {labels.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing epoch 1...\n",
            "Processing epoch 11...\n",
            "Processing epoch 21...\n",
            "Processing epoch 31...\n",
            "Processing epoch 41...\n",
            "Processing epoch 51...\n",
            "Processing epoch 61...\n",
            "Processing epoch 71...\n",
            "Processing epoch 81...\n",
            "Processing epoch 91...\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def l2_loss(y, yh):\n",
        "  return 0.5 * (yh - y)**2\n",
        "\n",
        "def l2_loss_grad(y, yh):\n",
        "  return yh - y\n",
        "\n",
        "def cross_entropy(y, yh):\n",
        "  return -np.sum(y * np.log(yh + 1e-12))\n",
        "\n",
        "# note that this is true only for dL/dz, L = loss(softmax(z))\n",
        "def cross_entropy_grad(y, yh):\n",
        "  return yh - y\n",
        "\n",
        "def relu(x):\n",
        "  return np.maximum(0, x)\n",
        "   \n",
        "def relu_grad(x):\n",
        "    return (x > 0).astype(float)\n",
        "\n",
        "def leaky_relu(x, alpha=0.1):\n",
        "   return np.maximum(alpha*x, x)\n",
        "\n",
        "def leaky_relu_grad(x, alpha=0.1):\n",
        "    grad = np.ones_like(x)\n",
        "    grad[x < 0] = alpha\n",
        "    return grad\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_grad(x):\n",
        "    s = sigmoid(x)\n",
        "    return s * (1 - s)\n",
        "  \n",
        "def tanh(x):\n",
        "  return np.tanh(x)\n",
        "\n",
        "def tanh_grad(x):\n",
        "  t = np.tanh(x)\n",
        "  return 1 - t * t\n",
        "\n",
        "def linear(x):\n",
        "  return x\n",
        "\n",
        "def linear_grad(x):\n",
        "  return np.ones_like(x)\n",
        "\n",
        "def softmax(x):\n",
        "  z = x - np.max(x)\n",
        "  e = np.exp(z)\n",
        "  return e / np.sum(e)\n",
        "\n",
        "# MLP object:\n",
        "# dims: list[int] - how many neurons in input, {hidden layers}, output\n",
        "# activation_fns: list[relu|sigmoid|tanh|linear|softmax] - activation functions applied to EACH HIDDEN LAYER\n",
        "# W: list[np.array(n_in x n_out)] - Random init with Normal dist.\n",
        "# b: list[np.array(n_out)] - Random init with Normal dist.\n",
        "# seed: float - For training reproducibility\n",
        "class MLP:\n",
        "  def __init__(self, dims, activation_fns, seed=None):\n",
        "    self.dims = dims\n",
        "    self.seed = seed\n",
        "    if seed:\n",
        "      np.random.seed(seed)\n",
        "\n",
        "    dims_len = len(dims)\n",
        "    activation_fns_len = len(activation_fns)\n",
        "    if dims_len - 1 != activation_fns_len:\n",
        "      dims_len = len(dims)\n",
        "      raise RuntimeError(f\"Length {dims_len} of dims does not match length {activation_fns_len} of activation_fns\")\n",
        "    \n",
        "    self.activation_fns = activation_fns\n",
        "\n",
        "    W_list = []\n",
        "    b_list = []\n",
        "    for i, dim in enumerate(dims[1:], start=1):\n",
        "      W_list.append(np.random.normal(loc=0.0, scale=1.0, size=(dims[i - 1], dims[i])))\n",
        "      b_list.append(np.zeros(dims[i]))\n",
        "    \n",
        "    self.W = W_list\n",
        "    self.b = b_list\n",
        "\n",
        "  def feed_forward(self, x):\n",
        "    Z = []\n",
        "    A = []\n",
        "    z = None # intermediate var init\n",
        "    a = np.array(x) # \"input activation\"\n",
        "    # will contain list[x, Vx, Wf(Vx), ...]\n",
        "    Z.append(x)\n",
        "    # will contain list[x, fn(Vx), fn(Wf(Vx)), ...]\n",
        "    A.append(x)\n",
        "    for i, (W, b, fn) in enumerate(zip(self.W, self.b, self.activation_fns), start=1):\n",
        "      z = a.T @ W + b\n",
        "      Z.append(z)\n",
        "      a = fn(z)\n",
        "      A.append(a)\n",
        "      z = a\n",
        "\n",
        "    return Z, A\n",
        "  \n",
        "  # def train(self, X, Y, batch_size, lr, epochs):\n",
        "  #   deltas = [] # deltas will be in same order as list of weight matrices/bias vectors\n",
        "  #   N = X.shape[0]\n",
        "  #   d = X.shape[1]\n",
        "  #   num_layers = len(self.dims)\n",
        "  #   train_batches = np.array_split(list(zip(X, Y)), N // batch_size)\n",
        "  #   for epoch in range(epochs):\n",
        "  #     for batch in train_batches:\n",
        "  #       weight_update_per_batch = []\n",
        "  #       bias_updates_per_batch = []\n",
        "  #       for x, y in batch:\n",
        "  #         deltas = []\n",
        "  #         Z, A = self.feed_forward(x)\n",
        "\n",
        "  #         last_activation_fn = self.activation_fns[len(self.activation_fns) - 1]\n",
        "  #         # classification output\n",
        "  #         if last_activation_fn is softmax:\n",
        "  #           delta_last = A[num_layers - 1] - y\n",
        "  #         # regression output\n",
        "  #         else:\n",
        "  #           delta_last = l2_loss_grad(A[num_layers - 1]) * activation_to_grad_map[last_activation_fn](Z[num_layers - 1])\n",
        "          \n",
        "  #         deltas.insert(0, delta_last)\n",
        "          \n",
        "  #         # move backwards (BACKprop)\n",
        "  #         # note that self.W and self.activation_fns (to_second_layer, to_third_layer, ...) are 1 shorter than Z (list[x, Vx, Wf(Vx), ...])\n",
        "  #         for layer in range(num_layers - 2, 0, -1):\n",
        "  #           activation_fn = self.activation_fns[layer]\n",
        "  #           delta = (self.W[layer].T @ deltas[0]) * activation_to_grad_map[activation_fn](Z[layer])\n",
        "  #           deltas.insert(0, delta)\n",
        "          \n",
        "  #         weight_updates_per_instance = []\n",
        "  #         bias_updates_per_instance = []\n",
        "  #         for i, delta in enumerate(deltas):\n",
        "  #           grad = A[i] @ delta\n",
        "  #           weight_updates_per_instance.append(grad / np.linalg.norm(grad))\n",
        "  #           bias_updates_per_instance.append(delta / np.linalg.norm(delta))\n",
        "          \n",
        "  #         weight_update_per_batch.append(weight_updates_per_instance)\n",
        "  #         bias_updates_per_batch.append(bias_updates_per_instance)\n",
        "        \n",
        "  #       for i, (selfW, selfB) in enumerate(self.W, self.b):\n",
        "  #         for (weight_batch_update, bias_batch_update) in zip(weight_update_per_batch, bias_updates_per_instance):\n",
        "  #           selfW -= lr * weight_batch_update[i]\n",
        "  #           selfB -= lr *bias_batch_update[i]\n",
        "          \n",
        "  #         self.W[i] = selfW\n",
        "  #         self.b[i] = selfB\n",
        "\n",
        "  def l1_penalty(self):\n",
        "    \"\"\"Sum of absolute values across a list of weight arrays.\"\"\"\n",
        "    total = 0.0\n",
        "    for W in self.W:\n",
        "        total += np.sum(np.abs(W))\n",
        "    return total\n",
        "\n",
        "  def l2_penalty(self):\n",
        "      \"\"\"Half the squared L2 norm across all weight arrays (0.5 * ||W||^2).\"\"\"\n",
        "      total = 0.0\n",
        "      for W in self.W:\n",
        "          total += 0.5 * np.sum(W * W)\n",
        "      return total\n",
        "  \n",
        "  def l1_grad(self, weights):\n",
        "      \"\"\"Gradient of L1 regularization: sign(W)\"\"\"\n",
        "      grads = []\n",
        "      for W in weights:\n",
        "          grads.append(np.sign(W))\n",
        "      return grads\n",
        "\n",
        "  def l2_grad(self, weights):\n",
        "      \"\"\"Gradient of L2 regularization: W\"\"\"\n",
        "      return weights  # Since d/dW (0.5 * W^2) = W\n",
        "  \n",
        "\n",
        "  def fit(self, X, Y, batch_size, lr, epochs, l1_lambda=0.0, l2_lambda=0.0):\n",
        "    \"\"\"\n",
        "    - Supports SGD/minibatch/full-batch via batch_size.\n",
        "    - Softmax+CE: δ_L = y_hat - y (no extra f').\n",
        "    - MSE (or any other): δ_L = (y_hat - y) ⊙ f'(z_L).\n",
        "    Assumes feed_forward(x) -> (Z, A) with:\n",
        "      A[0] = x, Z[1..L], A[L] = y_hat\n",
        "    \"\"\"\n",
        "    activation_to_grad_map = {\n",
        "        relu: relu_grad,\n",
        "        sigmoid: sigmoid_grad,\n",
        "        tanh: tanh_grad,\n",
        "        linear: linear_grad,\n",
        "        leaky_relu: leaky_relu_grad,\n",
        "    }\n",
        "    \n",
        "    N = X.shape[0]\n",
        "    L = len(self.dims) - 1  # number of weight layers\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"Processing epoch {epoch + 1}...\") if epoch % 10 == 0 else None\n",
        "        # shuffle each epoch\n",
        "        perm = np.random.permutation(N)\n",
        "        Xs, Ys = X[perm], Y[perm]\n",
        "\n",
        "        for start in range(0, N, batch_size):\n",
        "            end = min(start + batch_size, N)\n",
        "            m = end - start\n",
        "\n",
        "            # gradient accumulators\n",
        "            grad_W = [np.zeros_like(W) for W in self.W]\n",
        "            grad_b = [np.zeros_like(b) for b in self.b]\n",
        "\n",
        "            # per-sample backprop, sum then average\n",
        "            for x, y in zip(Xs[start:end], Ys[start:end]):\n",
        "                Z, A = self.feed_forward(x)  # A[0]=x, A[L]=ŷ; Z[1..L]\n",
        "                last_act = self.activation_fns[-1]\n",
        "\n",
        "                # delta per layer index: delta_layers[l] is deltal, for l=1..L. Means the array of deltas exactly match\n",
        "                # layers (means 0th delta is None because that's for input.)\n",
        "                delta_layers = [None] * (L + 1)\n",
        "\n",
        "                # Output layer delta\n",
        "                if last_act is softmax:\n",
        "                    delta_layers[L] = A[-1] - y\n",
        "                else:\n",
        "                    dz_L = activation_to_grad_map[last_act](Z[-1])  # f'(z_L)\n",
        "                    delta_layers[L] = (A[-1] - y) * dz_L\n",
        "\n",
        "                # Hidden layers: l = L-1 .. 1\n",
        "                for l in range(L - 1, 0, -1):\n",
        "                    act = self.activation_fns[l - 1]           # activation after W[l-1]\n",
        "                    dz = activation_to_grad_map[act](Z[l])      # f'(z_l)\n",
        "                    \n",
        "                    delta_layers[l] = (self.W[l] @ delta_layers[l + 1]) * dz\n",
        "\n",
        "                # Gradients for each weight layer i=0..L-1 (maps layer i -> i+1)\n",
        "                for i in range(L):\n",
        "                    # ∂L/∂W[i] = A[i] (col) ⊗ δ^{i+1} (row)\n",
        "                    grad_W[i] += np.outer(A[i], delta_layers[i + 1])\n",
        "                    # ∂L/∂b[i] = δ^{i+1}\n",
        "                    grad_b[i] += delta_layers[i + 1]\n",
        "\n",
        "            # Apply averaged batch gradients\n",
        "            inv_m = 1.0 / m\n",
        "            for i in range(L):\n",
        "                data_grad_W = grad_W[i] * inv_m\n",
        "                data_grad_b = grad_b[i] * inv_m\n",
        "                # Add regularization gradients\n",
        "                if l1_lambda > 0:\n",
        "                    data_grad_W += l1_lambda * np.sign(self.W[i])\n",
        "                if l2_lambda > 0:\n",
        "                    data_grad_W += l2_lambda * self.W[i]\n",
        "                \n",
        "                # Update weights\n",
        "                self.W[i] -= lr * data_grad_W\n",
        "                self.b[i] -= lr * data_grad_b\n",
        "            \n",
        "\n",
        "            if epoch % 10 == 0:\n",
        "              avg_loss = self._compute_batch_loss(Xs[:100], Ys[:100], l1_lambda, l2_lambda)\n",
        "              #print(f\"  Loss (with reg): {avg_loss:.4f}\")\n",
        "\n",
        "\n",
        "        \n",
        "        #print(self.loss(np.array([1,2,3,4,5]), np.array([1,1,1]))) if epoch % 10 == 0 else None\n",
        "                \n",
        "  def _compute_batch_loss(self, X_batch, Y_batch, l1_lambda=0.0, l2_lambda=0.0):\n",
        "      \"\"\"Compute average loss for a batch including regularization\"\"\"\n",
        "      total_loss = 0.0\n",
        "      for x, y in zip(X_batch, Y_batch):\n",
        "          total_loss += self.loss(x, y)\n",
        "      \n",
        "      avg_data_loss = total_loss / len(X_batch)\n",
        "      \n",
        "      # Add regularization terms\n",
        "      reg_loss = 0.0\n",
        "      if l1_lambda > 0:\n",
        "          reg_loss += l1_lambda * self.l1_penalty()\n",
        "      if l2_lambda > 0:\n",
        "          reg_loss += l2_lambda * self.l2_penalty() * 2  # Multiply by 2 because we use 0.5 in penalty\n",
        "      \n",
        "      return avg_data_loss + reg_loss\n",
        "  \n",
        "\n",
        "  def predict(self, x):\n",
        "     Z, A = self.feed_forward(x)\n",
        "     return np.argmax(A[-1]) # for softmax\n",
        "  \n",
        "  \n",
        "  def loss(self, x, y, reg=None):\n",
        "    last_act = self.activation_fns[-1]\n",
        "    Z, A = self.feed_forward(x)\n",
        "    \n",
        "    # CE loss\n",
        "    if last_act is softmax:\n",
        "      loss = cross_entropy(y, A[-1])\n",
        "    # L2 loss\n",
        "    else:\n",
        "      loss = np.sum(l2_loss(y, A[-1])) \n",
        "    return loss\n",
        "  \n",
        "  def evaluate_acc(self, X, y):\n",
        "        \"\"\"Calculate accuracy on dataset\"\"\"\n",
        "        correct = 0\n",
        "        for i in range(len(X)):\n",
        "            pred = self.predict(X[i])  # Use the predict method\n",
        "            if pred == y[i]:\n",
        "                correct += 1\n",
        "        return correct / len(X)\n",
        "     \n",
        "\n",
        "mlp = MLP([5,10,10,3], [sigmoid, linear, softmax])\n",
        "mlp.fit(np.array([np.array([1,2,3,4,5])]), np.array(np.array([1,1,1])), 1, 0.001, 100)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Python(35544) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
            "Python(35545) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
            "Python(35546) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
            "Python(35547) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
            "Python(35549) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
            "Python(35550) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing epoch 1...\n",
            "Model 1 - Train Accuracy: 0.5963, Test Accuracy: 0.5950\n",
            "Processing epoch 1...\n",
            "Model 2 - Train Accuracy: 0.6715, Test Accuracy: 0.6631\n",
            "Processing epoch 1...\n",
            "Model 3 - Train Accuracy: 0.7964, Test Accuracy: 0.7779\n"
          ]
        }
      ],
      "source": [
        "# 3.1\n",
        "\n",
        "def convert_loader_to_np(loader):\n",
        "    all_images = []\n",
        "    all_labels = []\n",
        "\n",
        "    for images, labels in loader:\n",
        "        images_flat = images.view(images.size(0), -1).numpy()\n",
        "        labels_np = labels.numpy()\n",
        "        all_images.append(images_flat)\n",
        "        all_labels.append(labels_np)\n",
        "\n",
        "    X = np.concatenate(all_images, axis=0)\n",
        "    y = np.concatenate(all_labels, axis=0)\n",
        "\n",
        "    return X, y\n",
        "\n",
        "def labels_to_onehot(y, num_classes=10):\n",
        "    \"\"\"\n",
        "    Convert integer labels to one-hot encoding\n",
        "    \"\"\"\n",
        "    onehot = np.zeros((len(y), num_classes))\n",
        "    onehot[np.arange(len(y)), y] = 1\n",
        "    return onehot\n",
        "\n",
        "train_loader, val_loader, test_loader, mean, std = get_fashion_mnist_loaders(\n",
        "        batch_size=128, \n",
        "        num_workers=2\n",
        "    )\n",
        "\n",
        "X_train, y_train = convert_loader_to_np(train_loader)\n",
        "y_train_onehot = labels_to_onehot(y_train)\n",
        "\n",
        "X_val, y_val = convert_loader_to_np(val_loader)\n",
        "y_val_onehot = labels_to_onehot(y_val)\n",
        "\n",
        "X_test, y_test = convert_loader_to_np(test_loader)\n",
        "y_test_onehot = labels_to_onehot(y_test)\n",
        "\n",
        "mlp_no_layers = MLP([784, 10], [softmax])\n",
        "mlp_no_layers.fit(X_train, y_train_onehot, 64, 0.001, 10)\n",
        "\n",
        "train_acc1 = mlp_no_layers.evaluate_acc(X_train, y_train)\n",
        "test_acc1 = mlp_no_layers.evaluate_acc(X_test, y_test)\n",
        "print(f\"Model 1 - Train Accuracy: {train_acc1:.4f}, Test Accuracy: {test_acc1:.4f}\")\n",
        "\n",
        "mlp_1_layer = MLP([784, 256, 10], [relu, softmax])\n",
        "mlp_2_layer = MLP([784, 256, 256, 10], [relu, relu, softmax])\n",
        "\n",
        "mlp_1_layer.fit(X_train, y_train_onehot, 64, 0.0001, 10)\n",
        "train_acc2 = mlp_1_layer.evaluate_acc(X_train, y_train)\n",
        "test_acc2 = mlp_1_layer.evaluate_acc(X_test, y_test)\n",
        "\n",
        "print(f\"Model 2 - Train Accuracy: {train_acc2:.4f}, Test Accuracy: {test_acc2:.4f}\")\n",
        "\n",
        "mlp_2_layer.fit(X_train, y_train_onehot, 64, 0.0001, 10)\n",
        "train_acc3 = mlp_2_layer.evaluate_acc(X_train, y_train)\n",
        "test_acc3 = mlp_2_layer.evaluate_acc(X_test, y_test)\n",
        "print(f\"Model 3 - Train Accuracy: {train_acc3:.4f}, Test Accuracy: {test_acc3:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing epoch 1...\n",
            "New model - Test accuracy: 0.7303\n"
          ]
        }
      ],
      "source": [
        "#3.2\n",
        "\n",
        "last_model_redo = MLP([784, 256, 256, 10], [tanh, leaky_relu, softmax])\n",
        "last_model_redo.fit(X_train, y_train_onehot, 64, 0.001, 10)\n",
        "test_acc = last_model_redo.evaluate_acc(X_test, y_test)\n",
        "print(f\"New model - Test accuracy: {test_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3.2 Analysis:\n",
        "With a low learning rate (0.0001), the new model performed worse than the original (accuracy 0.589). This is likely because the gradients from tanh already cause a diminished shift in the weights when learning.\n",
        "We increase the learning rate to 0.001 to verify our assumption that learning rate should be increased when using a network with sigmoid or tanh, since their gradients are very small.\n",
        "\n",
        "Our intuition is confirmed -> with a higher learning rate of 0.001, our test accuracy jumps to 0.7303!\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing epoch 1...\n",
            "New model - Test accuracy: 0.7732\n",
            "Processing epoch 1...\n",
            "New model - Test accuracy: 0.7742\n"
          ]
        }
      ],
      "source": [
        "# 3.3\n",
        "\n",
        "model1 = MLP([784, 256, 256, 10], [relu, relu, softmax])\n",
        "model1.fit(X_train, y_train_onehot, 64, 0.0001, 10, l1_lambda=0.001)\n",
        "test_acc = model1.evaluate_acc(X_test, y_test)\n",
        "print(f\"L1 - Test accuracy: {test_acc:.4f}\")\n",
        "\n",
        "model2 = MLP([784, 256, 256, 10], [relu, relu, softmax])\n",
        "model2.fit(X_train, y_train_onehot, 64, 0.0001, 10, l2_lambda=0.001)\n",
        "test_acc = model2.evaluate_acc(X_test, y_test)\n",
        "print(f\"L2 - Test accuracy: {test_acc:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3.3 analysis\n",
        "With regularization, our model performed similarly to without. This is likely because our model does not overfit.\n",
        "If, on the other hand, our model performed better on the test set with regularization, that would indicate that our model without regularization overfitted to the training data.\n",
        "Conclusion: Regularization decreases model accuracy very slightly IF it does not overfit. However, if it does overfit, it can increase test accuracy greatly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Python(35611) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
            "Python(35612) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
            "Python(35613) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
            "Python(35614) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing epoch 1...\n",
            "Unregularized Model - Train Accuracy: 0.7549, Test Accuracy: 0.7432\n",
            "Regularized Model (from 3.1) - Test Accuracy: 0.7779\n"
          ]
        }
      ],
      "source": [
        "#3.4\n",
        "\n",
        "# 3.4 - Train on unregularized images\n",
        "import torch\n",
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "\n",
        "def get_unregularized_fashion_mnist_loaders(batch_size=128, val_ratio=0.2, seed=551):\n",
        "    \"\"\"\n",
        "    Load FashionMNIST without normalization for question 3.4\n",
        "    \"\"\"\n",
        "    # Simple transform that only converts to tensor (no normalization)\n",
        "    basic_transform = transforms.Compose([\n",
        "        transforms.ToTensor()  # Only converts to [0,1], no normalization\n",
        "    ])\n",
        "    \n",
        "    full_train_dataset = datasets.FashionMNIST(\n",
        "        root=\"./data\",\n",
        "        train=True,\n",
        "        download=True,\n",
        "        transform=basic_transform\n",
        "    )\n",
        "\n",
        "    test_dataset = datasets.FashionMNIST(\n",
        "        root=\"./data\",\n",
        "        train=False,\n",
        "        download=True,\n",
        "        transform=basic_transform\n",
        "    )\n",
        "\n",
        "    # Split train into train/val\n",
        "    total_train = len(full_train_dataset)\n",
        "    val_size = int(val_ratio * total_train)\n",
        "    train_size = total_train - val_size\n",
        "\n",
        "    generator = torch.Generator().manual_seed(seed)\n",
        "    train_dataset, val_dataset = random_split(\n",
        "        full_train_dataset,\n",
        "        [train_size, val_size],\n",
        "        generator=generator\n",
        "    )\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=2\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=2\n",
        "    )\n",
        "\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=2\n",
        "    )\n",
        "\n",
        "    return train_loader, val_loader, test_loader\n",
        "\n",
        "#unregularized data\n",
        "train_loader_unreg, val_loader_unreg, test_loader_unreg = get_unregularized_fashion_mnist_loaders()\n",
        "\n",
        "# Convert to numpy arrays\n",
        "X_train_unreg, y_train_unreg = convert_loader_to_np(train_loader_unreg)\n",
        "y_train_onehot_unreg = labels_to_onehot(y_train_unreg)\n",
        "\n",
        "X_test_unreg, y_test_unreg = convert_loader_to_np(test_loader_unreg)\n",
        "\n",
        "# Train model on unregularized data\n",
        "model_unreg = MLP([784, 256, 256, 10], [relu, relu, softmax])\n",
        "model_unreg.fit(X_train_unreg, y_train_onehot_unreg, 64, 0.0001, 10)\n",
        "\n",
        "# Evaluate\n",
        "train_acc_unreg = model_unreg.evaluate_acc(X_train_unreg, y_train_unreg)\n",
        "test_acc_unreg = model_unreg.evaluate_acc(X_test_unreg, y_test_unreg)\n",
        "\n",
        "print(f\"Unregularized Model - Train Accuracy: {train_acc_unreg:.4f}, Test Accuracy: {test_acc_unreg:.4f}\")\n",
        "\n",
        "# Compare with regularized (normalized) model from 3.1\n",
        "print(f\"Regularized Model (from 3.1) - Test Accuracy: {test_acc3:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3.4 Analysis:\n",
        "the unregularized model performed worse than the regularized one. This is expected as regularization of data improves the dataset overall, which improves the training quality.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Python(36117) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
            "Python(36118) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
            "Python(36119) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
            "Python(36120) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing epoch 1...\n",
            "Processing epoch 11...\n",
            "Processing epoch 21...\n"
          ]
        }
      ],
      "source": [
        "# 3.5 - Data Augmentation\n",
        "\n",
        "def get_augmented_fashion_mnist_loaders(batch_size=128, val_ratio=0.2, seed=551):\n",
        "    \"\"\"\n",
        "    Load FashionMNIST with data augmentation for training\n",
        "    \"\"\"\n",
        "    # Compute mean and std for normalization (using your existing function)\n",
        "    mean, std = compute_fashion_mnist_mean_std()\n",
        "    \n",
        "    # Augmentation transforms for training\n",
        "    train_transform = transforms.Compose([\n",
        "        transforms.RandomHorizontalFlip(p=0.5),      # Random flip\n",
        "        transforms.RandomRotation(10),                # Random rotation ±10 degrees\n",
        "        transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),  # Random translation\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((mean,), (std,))\n",
        "    ])\n",
        "    \n",
        "    # Standard transforms for validation/test (no augmentation)\n",
        "    test_transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((mean,), (std,))\n",
        "    ])\n",
        "    \n",
        "    full_train_dataset = datasets.FashionMNIST(\n",
        "        root=\"./data\",\n",
        "        train=True,\n",
        "        download=True,\n",
        "        transform=train_transform  # Use augmented transforms for training\n",
        "    )\n",
        "\n",
        "    test_dataset = datasets.FashionMNIST(\n",
        "        root=\"./data\",\n",
        "        train=False,\n",
        "        download=True,\n",
        "        transform=test_transform\n",
        "    )\n",
        "\n",
        "    # Split train into train/val\n",
        "    total_train = len(full_train_dataset)\n",
        "    val_size = int(val_ratio * total_train)\n",
        "    train_size = total_train - val_size\n",
        "\n",
        "    generator = torch.Generator().manual_seed(seed)\n",
        "    train_dataset, val_dataset = random_split(\n",
        "        full_train_dataset,\n",
        "        [train_size, val_size],\n",
        "        generator=generator\n",
        "    )\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=2\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=2\n",
        "    )\n",
        "\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=2\n",
        "    )\n",
        "\n",
        "    return train_loader, val_loader, test_loader\n",
        "\n",
        "# Load augmented data\n",
        "train_loader_aug, val_loader_aug, test_loader_aug = get_augmented_fashion_mnist_loaders()\n",
        "\n",
        "# Convert to numpy arrays\n",
        "X_train_aug, y_train_aug = convert_loader_to_np(train_loader_aug)\n",
        "y_train_onehot_aug = labels_to_onehot(y_train_aug)\n",
        "\n",
        "X_test_aug, y_test_aug = convert_loader_to_np(test_loader_aug)\n",
        "\n",
        "# Train model on augmented data (using same architecture as 3.1)\n",
        "model_aug = MLP([784, 256, 256, 10], [relu, relu, softmax])\n",
        "model_aug.fit(X_train_aug, y_train_onehot_aug, 64, 0.0001, 40)\n",
        "\n",
        "# Evaluate\n",
        "train_acc_aug = model_aug.evaluate_acc(X_train_aug, y_train_aug)\n",
        "test_acc_aug = model_aug.evaluate_acc(X_test_aug, y_test_aug)\n",
        "\n",
        "print(f\"Augmented Model - Train Accuracy: {train_acc_aug:.4f}, Test Accuracy: {test_acc_aug:.4f}\")\n",
        "print(f\"Regular Model (from 3.1) - Train Accuracy: {train_acc3:.4f}, Test Accuracy: {test_acc3:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3.5: BROKEN, AUGMENTED DATA DOES WORSE??"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "d2l",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
