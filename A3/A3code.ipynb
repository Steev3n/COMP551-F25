{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U9V9UODBoVPh",
        "outputId": "e0212db5-09d7-4288-e596-152451dc370e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 26.4M/26.4M [00:01<00:00, 17.2MB/s]\n",
            "100%|██████████| 29.5k/29.5k [00:00<00:00, 278kB/s]\n",
            "100%|██████████| 4.42M/4.42M [00:00<00:00, 5.05MB/s]\n",
            "100%|██████████| 5.15k/5.15k [00:00<00:00, 9.43MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train batches: 375\n",
            "Validation batches: 94\n",
            "Test batches: 79\n",
            "Computed mean: 0.2860, std: 0.3530\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch image tensor shape: torch.Size([128, 1, 28, 28])\n",
            "Batch labels tensor shape: torch.Size([128])\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision import datasets, transforms\n",
        "import numpy as np\n",
        "\n",
        "def compute_fashion_mnist_mean_std(root=\"./data\"):\n",
        "    \"\"\"\n",
        "    Load raw FashionMNIST training data, convert to float in [0,1],\n",
        "    and compute global mean and std over all pixels, as recommended in CS231n:\n",
        "    center data to mean 0 and normalize its scale.\n",
        "    \"\"\"\n",
        "    # Load once without transforms to access raw uint8 data\n",
        "    raw_train = datasets.FashionMNIST(\n",
        "        root=root,\n",
        "        train=True,\n",
        "        download=True,\n",
        "        transform=None\n",
        "    )\n",
        "\n",
        "    # raw_train.data: shape [60000, 28, 28], dtype uint8 in [0, 255]\n",
        "    train_data = raw_train.data.float() / 255.0 # match ToTensor scaling\n",
        "\n",
        "    mean = train_data.mean().item()\n",
        "    std = train_data.std().item()\n",
        "    return mean, std\n",
        "\n",
        "\n",
        "def get_fashion_mnist_datasets(root=\"./data\", val_ratio=0.2, seed=551):\n",
        "    \"\"\"\n",
        "    Acquire FashionMNIST, compute normalization statistics on training set,\n",
        "    and return normalized train, validation, and test datasets.\n",
        "\n",
        "    - Uses the default 28x28 version.\n",
        "    - Uses the 60k official training split for train + validation.\n",
        "    - Uses the 10k official test split as test.\n",
        "    \"\"\"\n",
        "    mean, std = compute_fashion_mnist_mean_std(root)\n",
        "\n",
        "    train_transform = transforms.Compose([\n",
        "        transforms.ToTensor(), # [0, 255] -> [0, 1]\n",
        "        transforms.Normalize((mean,), (std,)) # zero mean, unit-ish variance\n",
        "    ])\n",
        "\n",
        "    test_transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((mean,), (std,))\n",
        "    ])\n",
        "\n",
        "    full_train_dataset = datasets.FashionMNIST(\n",
        "        root=root,\n",
        "        train=True,\n",
        "        download=True,\n",
        "        transform=train_transform\n",
        "    )\n",
        "\n",
        "    test_dataset = datasets.FashionMNIST(\n",
        "        root=root,\n",
        "        train=False,\n",
        "        download=True,\n",
        "        transform=test_transform\n",
        "    )\n",
        "\n",
        "    # Split 60k training samples into train and validation\n",
        "    total_train = len(full_train_dataset) # should be 60000\n",
        "    val_size = int(val_ratio * total_train)\n",
        "    train_size = total_train - val_size\n",
        "\n",
        "    generator = torch.Generator().manual_seed(seed)\n",
        "    train_dataset, val_dataset = random_split(\n",
        "        full_train_dataset,\n",
        "        [train_size, val_size],\n",
        "        generator=generator\n",
        "    )\n",
        "\n",
        "    return train_dataset, val_dataset, test_dataset, mean, std\n",
        "\n",
        "\n",
        "def get_fashion_mnist_loaders(\n",
        "    root=\"./data\",\n",
        "    val_ratio=0.2,\n",
        "    batch_size=128,\n",
        "    num_workers=2,\n",
        "    seed=551\n",
        "):\n",
        "    \"\"\"\n",
        "    Convenience function that wraps dataset acquisition and returns\n",
        "    DataLoaders for train, validation, and test sets.\n",
        "    \"\"\"\n",
        "    train_dataset, val_dataset, test_dataset, mean, std = get_fashion_mnist_datasets(\n",
        "        root=root,\n",
        "        val_ratio=val_ratio,\n",
        "        seed=seed\n",
        "    )\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    return train_loader, val_loader, test_loader, mean, std\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train_loader, val_loader, test_loader, mean, std = get_fashion_mnist_loaders()\n",
        "\n",
        "    print(f\"Train batches: {len(train_loader)}\")\n",
        "    print(f\"Validation batches: {len(val_loader)}\")\n",
        "    print(f\"Test batches: {len(test_loader)}\")\n",
        "    print(f\"Computed mean: {mean:.4f}, std: {std:.4f}\")\n",
        "\n",
        "    # Inspect one batch shape\n",
        "    images, labels = next(iter(train_loader))\n",
        "    # images shape: [batch_size, 1, 28, 28]\n",
        "    print(f\"Batch image tensor shape: {images.shape}\")\n",
        "    print(f\"Batch labels tensor shape: {labels.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing epoch 1...\n",
            "14.971689027023572\n",
            "Processing epoch 11...\n",
            "12.747667704691425\n",
            "Processing epoch 21...\n",
            "10.718051934971527\n",
            "Processing epoch 31...\n",
            "9.063211893814346\n",
            "Processing epoch 41...\n",
            "7.936221010728584\n",
            "Processing epoch 51...\n",
            "7.26028014209837\n",
            "Processing epoch 61...\n",
            "6.824966003114476\n",
            "Processing epoch 71...\n",
            "6.479547909048205\n",
            "Processing epoch 81...\n",
            "6.154880555780511\n",
            "Processing epoch 91...\n",
            "5.825663412492704\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def l2_loss(y, yh):\n",
        "  return 0.5 * (yh - y)**2\n",
        "\n",
        "def l2_loss_grad(y, yh):\n",
        "  return yh - y\n",
        "\n",
        "def cross_entropy(y, yh):\n",
        "  return -np.sum(y * np.log(yh + 1e-12))\n",
        "\n",
        "# note that this is true only for dL/dz, L = loss(softmax(z))\n",
        "def cross_entropy_grad(y, yh):\n",
        "  return yh - y\n",
        "\n",
        "def relu(x):\n",
        "  return np.maximum(0, x)\n",
        "\n",
        "def relu_grad(x):\n",
        "    return (x > 0).astype(float)\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_grad(x):\n",
        "    s = sigmoid(x)\n",
        "    return s * (1 - s)\n",
        "  \n",
        "def tanh(x):\n",
        "  return np.tanh(x)\n",
        "\n",
        "def tanh_grad(x):\n",
        "  t = np.tanh(x)\n",
        "  return 1 - t * t\n",
        "\n",
        "def linear(x):\n",
        "  return x\n",
        "\n",
        "def linear_grad(x):\n",
        "  return np.ones_like(len(x))\n",
        "\n",
        "def softmax(x):\n",
        "  z = x - np.max(x)\n",
        "  e = np.exp(z)\n",
        "  return e / np.sum(e)\n",
        "\n",
        "# MLP object:\n",
        "# dims: list[int] - how many neurons in input, {hidden layers}, output\n",
        "# activation_fns: list[relu|sigmoid|tanh|linear|softmax] - activation functions applied to EACH HIDDEN LAYER\n",
        "# W: list[np.array(n_in x n_out)] - Random init with Normal dist.\n",
        "# b: list[np.array(n_out)] - Random init with Normal dist.\n",
        "# seed: float - For training reproducibility\n",
        "class MLP:\n",
        "  def __init__(self, dims, activation_fns, seed=None):\n",
        "    self.dims = dims\n",
        "    self.seed = seed\n",
        "    if seed:\n",
        "      np.random.seed(seed)\n",
        "\n",
        "    dims_len = len(dims)\n",
        "    activation_fns_len = len(activation_fns)\n",
        "    if dims_len - 1 != activation_fns_len:\n",
        "      dims_len = len(dims)\n",
        "      raise RuntimeError(f\"Length {dims_len} of dims does not match length {activation_fns_len} of activation_fns\")\n",
        "    \n",
        "    self.activation_fns = activation_fns\n",
        "\n",
        "    W_list = []\n",
        "    b_list = []\n",
        "    for i, dim in enumerate(dims[1:], start=1):\n",
        "      W_list.append(np.random.normal(loc=0.0, scale=1.0, size=(dims[i - 1], dims[i])))\n",
        "      b_list.append(np.zeros(dims[i]))\n",
        "    \n",
        "    self.W = W_list\n",
        "    self.b = b_list\n",
        "\n",
        "  def feed_forward(self, x):\n",
        "    Z = []\n",
        "    A = []\n",
        "    z = None # intermediate var init\n",
        "    a = np.array(x) # \"input activation\"\n",
        "    # will contain list[x, Vx, Wf(Vx), ...]\n",
        "    Z.append(x)\n",
        "    # will contain list[x, fn(Vx), fn(Wf(Vx)), ...]\n",
        "    A.append(x)\n",
        "    for i, (W, b, fn) in enumerate(zip(self.W, self.b, self.activation_fns), start=1):\n",
        "      z = a.T @ W + b\n",
        "      Z.append(z)\n",
        "      a = fn(z)\n",
        "      A.append(a)\n",
        "      z = a\n",
        "\n",
        "    return Z, A\n",
        "  \n",
        "  # def train(self, X, Y, batch_size, lr, epochs):\n",
        "  #   deltas = [] # deltas will be in same order as list of weight matrices/bias vectors\n",
        "  #   N = X.shape[0]\n",
        "  #   d = X.shape[1]\n",
        "  #   num_layers = len(self.dims)\n",
        "  #   train_batches = np.array_split(list(zip(X, Y)), N // batch_size)\n",
        "  #   for epoch in range(epochs):\n",
        "  #     for batch in train_batches:\n",
        "  #       weight_update_per_batch = []\n",
        "  #       bias_updates_per_batch = []\n",
        "  #       for x, y in batch:\n",
        "  #         deltas = []\n",
        "  #         Z, A = self.feed_forward(x)\n",
        "\n",
        "  #         last_activation_fn = self.activation_fns[len(self.activation_fns) - 1]\n",
        "  #         # classification output\n",
        "  #         if last_activation_fn is softmax:\n",
        "  #           delta_last = A[num_layers - 1] - y\n",
        "  #         # regression output\n",
        "  #         else:\n",
        "  #           delta_last = l2_loss_grad(A[num_layers - 1]) * activation_to_grad_map[last_activation_fn](Z[num_layers - 1])\n",
        "          \n",
        "  #         deltas.insert(0, delta_last)\n",
        "          \n",
        "  #         # move backwards (BACKprop)\n",
        "  #         # note that self.W and self.activation_fns (to_second_layer, to_third_layer, ...) are 1 shorter than Z (list[x, Vx, Wf(Vx), ...])\n",
        "  #         for layer in range(num_layers - 2, 0, -1):\n",
        "  #           activation_fn = self.activation_fns[layer]\n",
        "  #           delta = (self.W[layer].T @ deltas[0]) * activation_to_grad_map[activation_fn](Z[layer])\n",
        "  #           deltas.insert(0, delta)\n",
        "          \n",
        "  #         weight_updates_per_instance = []\n",
        "  #         bias_updates_per_instance = []\n",
        "  #         for i, delta in enumerate(deltas):\n",
        "  #           grad = A[i] @ delta\n",
        "  #           weight_updates_per_instance.append(grad / np.linalg.norm(grad))\n",
        "  #           bias_updates_per_instance.append(delta / np.linalg.norm(delta))\n",
        "          \n",
        "  #         weight_update_per_batch.append(weight_updates_per_instance)\n",
        "  #         bias_updates_per_batch.append(bias_updates_per_instance)\n",
        "        \n",
        "  #       for i, (selfW, selfB) in enumerate(self.W, self.b):\n",
        "  #         for (weight_batch_update, bias_batch_update) in zip(weight_update_per_batch, bias_updates_per_instance):\n",
        "  #           selfW -= lr * weight_batch_update[i]\n",
        "  #           selfB -= lr *bias_batch_update[i]\n",
        "          \n",
        "  #         self.W[i] = selfW\n",
        "  #         self.b[i] = selfB\n",
        "\n",
        "  def train(self, X, Y, batch_size, lr, epochs):\n",
        "    \"\"\"\n",
        "    - Supports SGD/minibatch/full-batch via batch_size.\n",
        "    - Softmax+CE: δ_L = y_hat - y (no extra f').\n",
        "    - MSE (or any other): δ_L = (y_hat - y) ⊙ f'(z_L).\n",
        "    Assumes feed_forward(x) -> (Z, A) with:\n",
        "      A[0] = x, Z[1..L], A[L] = y_hat\n",
        "    \"\"\"\n",
        "    activation_to_grad_map = {\n",
        "        relu: relu_grad,\n",
        "        sigmoid: sigmoid_grad,\n",
        "        tanh: tanh_grad,\n",
        "        linear: linear_grad,\n",
        "    }\n",
        "    \n",
        "    N = X.shape[0]\n",
        "    L = len(self.dims) - 1  # number of weight layers\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"Processing epoch {epoch + 1}...\") if epoch % 10 == 0 else None\n",
        "        # shuffle each epoch\n",
        "        perm = np.random.permutation(N)\n",
        "        Xs, Ys = X[perm], Y[perm]\n",
        "\n",
        "        for start in range(0, N, batch_size):\n",
        "            end = min(start + batch_size, N)\n",
        "            m = end - start\n",
        "\n",
        "            # gradient accumulators\n",
        "            grad_W = [np.zeros_like(W) for W in self.W]\n",
        "            grad_b = [np.zeros_like(b) for b in self.b]\n",
        "\n",
        "            # per-sample backprop, sum then average\n",
        "            for x, y in zip(Xs[start:end], Ys[start:end]):\n",
        "                Z, A = self.feed_forward(x)  # A[0]=x, A[L]=ŷ; Z[1..L]\n",
        "                last_act = self.activation_fns[-1]\n",
        "\n",
        "                # delta per layer index: delta_layers[l] is deltal, for l=1..L. Means the array of deltas exactly match\n",
        "                # layers (means 0th delta is None because that's for input.)\n",
        "                delta_layers = [None] * (L + 1)\n",
        "\n",
        "                # Output layer delta\n",
        "                if last_act is softmax:\n",
        "                    delta_layers[L] = A[-1] - y\n",
        "                else:\n",
        "                    dz_L = activation_to_grad_map[last_act](Z[-1])  # f'(z_L)\n",
        "                    delta_layers[L] = (A[-1] - y) * dz_L\n",
        "\n",
        "                # Hidden layers: l = L-1 .. 1\n",
        "                for l in range(L - 1, 0, -1):\n",
        "                    act = self.activation_fns[l - 1]           # activation after W[l-1]\n",
        "                    dz = activation_to_grad_map[act](Z[l])      # f'(z_l)\n",
        "                    \n",
        "                    delta_layers[l] = (self.W[l] @ delta_layers[l + 1]) * dz\n",
        "\n",
        "                # Gradients for each weight layer i=0..L-1 (maps layer i -> i+1)\n",
        "                for i in range(L):\n",
        "                    # ∂L/∂W[i] = A[i] (col) ⊗ δ^{i+1} (row)\n",
        "                    grad_W[i] += np.outer(A[i], delta_layers[i + 1])\n",
        "                    # ∂L/∂b[i] = δ^{i+1}\n",
        "                    grad_b[i] += delta_layers[i + 1]\n",
        "\n",
        "            # Apply averaged batch gradients\n",
        "            inv_m = 1.0 / m\n",
        "            for i in range(L):\n",
        "                self.W[i] -= lr * (grad_W[i] * inv_m)\n",
        "                self.b[i] -= lr * (grad_b[i] * inv_m)\n",
        "        \n",
        "        print(self.loss(np.array([1,2,3,4,5]), np.array([1,1,1]))) if epoch % 10 == 0 else None\n",
        "  \n",
        "  def loss(self, x, y):\n",
        "    last_act = self.activation_fns[-1]\n",
        "    Z, A = self.feed_forward(x)\n",
        "    \n",
        "    # CE loss\n",
        "    if last_act is softmax:\n",
        "      loss = cross_entropy(y, A[-1])\n",
        "    # L2 loss\n",
        "    else:\n",
        "      loss = l2_loss(y, A[-1])\n",
        "    \n",
        "    return loss\n",
        "     \n",
        "\n",
        "mlp = MLP([5,10,10,3], [sigmoid, linear, softmax])\n",
        "mlp.train(np.array([np.array([1,2,3,4,5])]), np.array(np.array([1,1,1])), 1, 0.001, 100)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "d2l",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
