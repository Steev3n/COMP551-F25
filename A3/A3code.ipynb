{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U9V9UODBoVPh",
        "outputId": "e0212db5-09d7-4288-e596-152451dc370e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 26.4M/26.4M [00:01<00:00, 20.0MB/s]\n",
            "100%|██████████| 29.5k/29.5k [00:00<00:00, 339kB/s]\n",
            "100%|██████████| 4.42M/4.42M [00:00<00:00, 6.32MB/s]\n",
            "100%|██████████| 5.15k/5.15k [00:00<00:00, 17.2MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train batches: 375\n",
            "Validation batches: 94\n",
            "Test batches: 79\n",
            "Computed mean: 0.2860, std: 0.3530\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch image tensor shape: torch.Size([128, 1, 28, 28])\n",
            "Batch labels tensor shape: torch.Size([128])\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, random_split, Subset\n",
        "from torchvision import datasets, transforms\n",
        "import numpy as np\n",
        "\n",
        "def compute_fashion_mnist_mean_std(root=\"./data\"):\n",
        "    \"\"\"\n",
        "    Load raw FashionMNIST training data, convert to float in [0,1],\n",
        "    and compute global mean and std over all pixels, as recommended in CS231n:\n",
        "    center data to mean 0 and normalize its scale.\n",
        "    \"\"\"\n",
        "    # Load once without transforms to access raw uint8 data\n",
        "    raw_train = datasets.FashionMNIST(\n",
        "        root=root,\n",
        "        train=True,\n",
        "        download=True,\n",
        "        transform=None\n",
        "    )\n",
        "\n",
        "    # raw_train.data: shape [60000, 28, 28], dtype uint8 in [0, 255]\n",
        "    train_data = raw_train.data.float() / 255.0 # match ToTensor scaling\n",
        "\n",
        "    mean = train_data.mean().item()\n",
        "    std = train_data.std().item()\n",
        "    return mean, std\n",
        "\n",
        "\n",
        "def get_fashion_mnist_datasets(root=\"./data\", val_ratio=0.2, seed=551):\n",
        "    \"\"\"\n",
        "    Acquire FashionMNIST, compute normalization statistics on training set,\n",
        "    and return normalized train, validation, and test datasets.\n",
        "\n",
        "    - Uses the default 28x28 version.\n",
        "    - Uses the 60k official training split for train + validation.\n",
        "    - Uses the 10k official test split as test.\n",
        "    \"\"\"\n",
        "    mean, std = compute_fashion_mnist_mean_std(root)\n",
        "\n",
        "    train_transform = transforms.Compose([\n",
        "        transforms.ToTensor(), # [0, 255] -> [0, 1]\n",
        "        transforms.Normalize((mean,), (std,)) # zero mean, unit-ish variance\n",
        "    ])\n",
        "\n",
        "    test_transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((mean,), (std,))\n",
        "    ])\n",
        "\n",
        "    full_train_dataset = datasets.FashionMNIST(\n",
        "        root=root,\n",
        "        train=True,\n",
        "        download=True,\n",
        "        transform=train_transform\n",
        "    )\n",
        "\n",
        "    test_dataset = datasets.FashionMNIST(\n",
        "        root=root,\n",
        "        train=False,\n",
        "        download=True,\n",
        "        transform=test_transform\n",
        "    )\n",
        "\n",
        "    # Split 60k training samples into train and validation\n",
        "    total_train = len(full_train_dataset) # should be 60000\n",
        "    val_size = int(val_ratio * total_train)\n",
        "    train_size = total_train - val_size\n",
        "\n",
        "    generator = torch.Generator().manual_seed(seed)\n",
        "    train_dataset, val_dataset = random_split(\n",
        "        full_train_dataset,\n",
        "        [train_size, val_size],\n",
        "        generator=generator\n",
        "    )\n",
        "\n",
        "    return train_dataset, val_dataset, test_dataset, mean, std\n",
        "\n",
        "\n",
        "def get_fashion_mnist_loaders(\n",
        "    root=\"./data\",\n",
        "    val_ratio=0.2,\n",
        "    batch_size=128,\n",
        "    num_workers=2,\n",
        "    seed=551\n",
        "):\n",
        "    \"\"\"\n",
        "    Convenience function that wraps dataset acquisition and returns\n",
        "    DataLoaders for train, validation, and test sets.\n",
        "    \"\"\"\n",
        "    train_dataset, val_dataset, test_dataset, mean, std = get_fashion_mnist_datasets(\n",
        "        root=root,\n",
        "        val_ratio=val_ratio,\n",
        "        seed=seed\n",
        "    )\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    return train_loader, val_loader, test_loader, mean, std\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train_loader, val_loader, test_loader, mean, std = get_fashion_mnist_loaders()\n",
        "\n",
        "    print(f\"Train batches: {len(train_loader)}\")\n",
        "    print(f\"Validation batches: {len(val_loader)}\")\n",
        "    print(f\"Test batches: {len(test_loader)}\")\n",
        "    print(f\"Computed mean: {mean:.4f}, std: {std:.4f}\")\n",
        "\n",
        "    # Inspect one batch shape\n",
        "    images, labels = next(iter(train_loader))\n",
        "    # images shape: [batch_size, 1, 28, 28]\n",
        "    print(f\"Batch image tensor shape: {images.shape}\")\n",
        "    print(f\"Batch labels tensor shape: {labels.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing epoch 1...\n",
            "Processing epoch 11...\n",
            "Processing epoch 21...\n",
            "Processing epoch 31...\n",
            "Processing epoch 41...\n",
            "Processing epoch 51...\n",
            "Processing epoch 61...\n",
            "Processing epoch 71...\n",
            "Processing epoch 81...\n",
            "Processing epoch 91...\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def l2_loss(y, yh):\n",
        "  return 0.5 * (yh - y)**2\n",
        "\n",
        "def l2_loss_grad(y, yh):\n",
        "  return yh - y\n",
        "\n",
        "def cross_entropy(y, yh):\n",
        "  return -np.sum(y * np.log(yh + 1e-12))\n",
        "\n",
        "# note that this is true only for dL/dz, L = loss(softmax(z))\n",
        "def cross_entropy_grad(y, yh):\n",
        "  return yh - y\n",
        "\n",
        "def relu(x):\n",
        "  return np.maximum(0, x)\n",
        "   \n",
        "def relu_grad(x):\n",
        "    return (x > 0).astype(float)\n",
        "\n",
        "def leaky_relu(x, alpha=0.1):\n",
        "   return np.maximum(alpha*x, x)\n",
        "\n",
        "def leaky_relu_grad(x, alpha=0.1):\n",
        "    grad = np.ones_like(x)\n",
        "    grad[x < 0] = alpha\n",
        "    return grad\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_grad(x):\n",
        "    s = sigmoid(x)\n",
        "    return s * (1 - s)\n",
        "  \n",
        "def tanh(x):\n",
        "  return np.tanh(x)\n",
        "\n",
        "def tanh_grad(x):\n",
        "  t = np.tanh(x)\n",
        "  return 1 - t * t\n",
        "\n",
        "def linear(x):\n",
        "  return x\n",
        "\n",
        "def linear_grad(x):\n",
        "  return np.ones_like(x)\n",
        "\n",
        "def softmax(x):\n",
        "  z = x - np.max(x)\n",
        "  e = np.exp(z)\n",
        "  return e / np.sum(e)\n",
        "\n",
        "# MLP object:\n",
        "# dims: list[int] - how many neurons in input, {hidden layers}, output\n",
        "# activation_fns: list[relu|sigmoid|tanh|linear|softmax] - activation functions applied to EACH HIDDEN LAYER\n",
        "# W: list[np.array(n_in x n_out)] - Random init with Normal dist.\n",
        "# b: list[np.array(n_out)] - Random init with Normal dist.\n",
        "# seed: float - For training reproducibility\n",
        "class MLP:\n",
        "  def __init__(self, dims, activation_fns, seed=None):\n",
        "    self.dims = dims\n",
        "    self.seed = seed\n",
        "    if seed:\n",
        "      np.random.seed(seed)\n",
        "\n",
        "    dims_len = len(dims)\n",
        "    activation_fns_len = len(activation_fns)\n",
        "    if dims_len - 1 != activation_fns_len:\n",
        "      dims_len = len(dims)\n",
        "      raise RuntimeError(f\"Length {dims_len} of dims does not match length {activation_fns_len} of activation_fns\")\n",
        "    \n",
        "    self.activation_fns = activation_fns\n",
        "\n",
        "    W_list = []\n",
        "    b_list = []\n",
        "    for i, dim in enumerate(dims[1:], start=1):\n",
        "      W_list.append(np.random.normal(loc=0.0, scale=1.0, size=(dims[i - 1], dims[i])))\n",
        "      b_list.append(np.zeros(dims[i]))\n",
        "    \n",
        "    self.W = W_list\n",
        "    self.b = b_list\n",
        "\n",
        "  def feed_forward(self, x):\n",
        "    Z = []\n",
        "    A = []\n",
        "    z = None # intermediate var init\n",
        "    a = np.array(x) # \"input activation\"\n",
        "    # will contain list[x, Vx, Wf(Vx), ...]\n",
        "    Z.append(x)\n",
        "    # will contain list[x, fn(Vx), fn(Wf(Vx)), ...]\n",
        "    A.append(x)\n",
        "    for i, (W, b, fn) in enumerate(zip(self.W, self.b, self.activation_fns), start=1):\n",
        "      z = a.T @ W + b\n",
        "      Z.append(z)\n",
        "      a = fn(z)\n",
        "      A.append(a)\n",
        "      z = a\n",
        "\n",
        "    return Z, A\n",
        "\n",
        "  def l1_penalty(self):\n",
        "    \"\"\"Sum of absolute values across a list of weight arrays.\"\"\"\n",
        "    total = 0.0\n",
        "    for W in self.W:\n",
        "        total += np.sum(np.abs(W))\n",
        "    return total\n",
        "\n",
        "  def l2_penalty(self):\n",
        "      \"\"\"Half the squared L2 norm across all weight arrays (0.5 * ||W||^2).\"\"\"\n",
        "      total = 0.0\n",
        "      for W in self.W:\n",
        "          total += 0.5 * np.sum(W * W)\n",
        "      return total\n",
        "  \n",
        "  def l1_grad(self, weights):\n",
        "      \"\"\"Gradient of L1 regularization: sign(W)\"\"\"\n",
        "      grads = []\n",
        "      for W in weights:\n",
        "          grads.append(np.sign(W))\n",
        "      return grads\n",
        "\n",
        "  def l2_grad(self, weights):\n",
        "      \"\"\"Gradient of L2 regularization: W\"\"\"\n",
        "      return weights  # Since d/dW (0.5 * W^2) = W\n",
        "  \n",
        "\n",
        "  def fit(self, X, Y, batch_size, lr, epochs, l1_lambda=0.0, l2_lambda=0.0):\n",
        "    \"\"\"\n",
        "    - Supports SGD/minibatch/full-batch via batch_size.\n",
        "    - Softmax+CE: δ_L = y_hat - y (no extra f').\n",
        "    - MSE (or any other): δ_L = (y_hat - y) ⊙ f'(z_L).\n",
        "    Assumes feed_forward(x) -> (Z, A) with:\n",
        "      A[0] = x, Z[1..L], A[L] = y_hat\n",
        "    \"\"\"\n",
        "    activation_to_grad_map = {\n",
        "        relu: relu_grad,\n",
        "        sigmoid: sigmoid_grad,\n",
        "        tanh: tanh_grad,\n",
        "        linear: linear_grad,\n",
        "        leaky_relu: leaky_relu_grad,\n",
        "    }\n",
        "    \n",
        "    N = X.shape[0]\n",
        "    L = len(self.dims) - 1  # number of weight layers\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"Processing epoch {epoch + 1}...\") if epoch % 10 == 0 else None\n",
        "        # shuffle each epoch\n",
        "        perm = np.random.permutation(N)\n",
        "        Xs, Ys = X[perm], Y[perm]\n",
        "\n",
        "        for start in range(0, N, batch_size):\n",
        "            end = min(start + batch_size, N)\n",
        "            m = end - start\n",
        "\n",
        "            # gradient accumulators\n",
        "            grad_W = [np.zeros_like(W) for W in self.W]\n",
        "            grad_b = [np.zeros_like(b) for b in self.b]\n",
        "\n",
        "            # per-sample backprop, sum then average\n",
        "            for x, y in zip(Xs[start:end], Ys[start:end]):\n",
        "                Z, A = self.feed_forward(x)  # A[0]=x, A[L]=ŷ; Z[1..L]\n",
        "                last_act = self.activation_fns[-1]\n",
        "\n",
        "                # delta per layer index: delta_layers[l] is deltal, for l=1..L. Means the array of deltas exactly match\n",
        "                # layers (means 0th delta is None because that's for input.)\n",
        "                delta_layers = [None] * (L + 1)\n",
        "\n",
        "                # Output layer delta\n",
        "                if last_act is softmax:\n",
        "                    delta_layers[L] = A[-1] - y\n",
        "                else:\n",
        "                    dz_L = activation_to_grad_map[last_act](Z[-1])  # f'(z_L)\n",
        "                    delta_layers[L] = (A[-1] - y) * dz_L\n",
        "\n",
        "                # Hidden layers: l = L-1 .. 1\n",
        "                for l in range(L - 1, 0, -1):\n",
        "                    act = self.activation_fns[l - 1]           # activation after W[l-1]\n",
        "                    dz = activation_to_grad_map[act](Z[l])      # f'(z_l)\n",
        "                    \n",
        "                    delta_layers[l] = (self.W[l] @ delta_layers[l + 1]) * dz\n",
        "\n",
        "                # Gradients for each weight layer i=0..L-1 (maps layer i -> i+1)\n",
        "                for i in range(L):\n",
        "                    # ∂L/∂W[i] = A[i] (col) ⊗ δ^{i+1} (row)\n",
        "                    grad_W[i] += np.outer(A[i], delta_layers[i + 1])\n",
        "                    # ∂L/∂b[i] = δ^{i+1}\n",
        "                    grad_b[i] += delta_layers[i + 1]\n",
        "\n",
        "            # Apply averaged batch gradients\n",
        "            inv_m = 1.0 / m\n",
        "            for i in range(L):\n",
        "                data_grad_W = grad_W[i] * inv_m\n",
        "                data_grad_b = grad_b[i] * inv_m\n",
        "                # Add regularization gradients\n",
        "                if l1_lambda > 0:\n",
        "                    data_grad_W += l1_lambda * np.sign(self.W[i])\n",
        "                if l2_lambda > 0:\n",
        "                    data_grad_W += l2_lambda * self.W[i]\n",
        "                \n",
        "                # Update weights\n",
        "                self.W[i] -= lr * data_grad_W\n",
        "                self.b[i] -= lr * data_grad_b\n",
        "            \n",
        "\n",
        "            if epoch % 10 == 0:\n",
        "              avg_loss = self._compute_batch_loss(Xs[:100], Ys[:100], l1_lambda, l2_lambda)\n",
        "              #print(f\"  Loss (with reg): {avg_loss:.4f}\")\n",
        "\n",
        "                \n",
        "  def _compute_batch_loss(self, X_batch, Y_batch, l1_lambda=0.0, l2_lambda=0.0):\n",
        "      \"\"\"Compute average loss for a batch including regularization\"\"\"\n",
        "      total_loss = 0.0\n",
        "      for x, y in zip(X_batch, Y_batch):\n",
        "          total_loss += self.loss(x, y)\n",
        "      \n",
        "      avg_data_loss = total_loss / len(X_batch)\n",
        "      \n",
        "      # Add regularization terms\n",
        "      reg_loss = 0.0\n",
        "      if l1_lambda > 0:\n",
        "          reg_loss += l1_lambda * self.l1_penalty()\n",
        "      if l2_lambda > 0:\n",
        "          reg_loss += l2_lambda * self.l2_penalty() * 2  # Multiply by 2 because we use 0.5 in penalty\n",
        "      \n",
        "      return avg_data_loss + reg_loss\n",
        "  \n",
        "\n",
        "  def predict(self, x):\n",
        "     Z, A = self.feed_forward(x)\n",
        "     return np.argmax(A[-1]) # for softmax\n",
        "  \n",
        "  \n",
        "  def loss(self, x, y, reg=None):\n",
        "    last_act = self.activation_fns[-1]\n",
        "    Z, A = self.feed_forward(x)\n",
        "    \n",
        "    # CE loss\n",
        "    if last_act is softmax:\n",
        "      loss = cross_entropy(y, A[-1])\n",
        "    # L2 loss\n",
        "    else:\n",
        "      loss = np.sum(l2_loss(y, A[-1])) \n",
        "    return loss\n",
        "  \n",
        "  def evaluate_acc(self, X, y):\n",
        "        \"\"\"Calculate accuracy on dataset\"\"\"\n",
        "        correct = 0\n",
        "        for i in range(len(X)):\n",
        "            pred = self.predict(X[i])  # Use the predict method\n",
        "            if pred == y[i]:\n",
        "                correct += 1\n",
        "        return correct / len(X)\n",
        "     \n",
        "\n",
        "mlp = MLP([5,10,10,3], [sigmoid, linear, softmax])\n",
        "mlp.fit(np.array([np.array([1,2,3,4,5])]), np.array(np.array([1,1,1])), 1, 0.001, 100)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing epoch 1...\n",
            "Model 1 - Train Accuracy: 0.6255, Test Accuracy: 0.6173\n",
            "Processing epoch 1...\n",
            "Model 2 - Train Accuracy: 0.6538, Test Accuracy: 0.6492\n",
            "Processing epoch 1...\n",
            "Model 3 - Train Accuracy: 0.8033, Test Accuracy: 0.7818\n"
          ]
        }
      ],
      "source": [
        "# 3.1\n",
        "\n",
        "def convert_loader_to_np(loader):\n",
        "    all_images = []\n",
        "    all_labels = []\n",
        "\n",
        "    for images, labels in loader:\n",
        "        images_flat = images.view(images.size(0), -1).numpy()\n",
        "        labels_np = labels.numpy()\n",
        "        all_images.append(images_flat)\n",
        "        all_labels.append(labels_np)\n",
        "\n",
        "    X = np.concatenate(all_images, axis=0)\n",
        "    y = np.concatenate(all_labels, axis=0)\n",
        "\n",
        "    return X, y\n",
        "\n",
        "def labels_to_onehot(y, num_classes=10):\n",
        "    \"\"\"\n",
        "    Convert integer labels to one-hot encoding\n",
        "    \"\"\"\n",
        "    onehot = np.zeros((len(y), num_classes))\n",
        "    onehot[np.arange(len(y)), y] = 1\n",
        "    return onehot\n",
        "\n",
        "train_loader, val_loader, test_loader, mean, std = get_fashion_mnist_loaders(\n",
        "        batch_size=128, \n",
        "        num_workers=2\n",
        "    )\n",
        "\n",
        "X_train, y_train = convert_loader_to_np(train_loader)\n",
        "y_train_onehot = labels_to_onehot(y_train)\n",
        "\n",
        "X_val, y_val = convert_loader_to_np(val_loader)\n",
        "y_val_onehot = labels_to_onehot(y_val)\n",
        "\n",
        "X_test, y_test = convert_loader_to_np(test_loader)\n",
        "y_test_onehot = labels_to_onehot(y_test)\n",
        "\n",
        "mlp_no_layers = MLP([784, 10], [softmax])\n",
        "mlp_no_layers.fit(X_train, y_train_onehot, 64, 0.001, 10)\n",
        "\n",
        "train_acc1 = mlp_no_layers.evaluate_acc(X_train, y_train)\n",
        "test_acc1 = mlp_no_layers.evaluate_acc(X_test, y_test)\n",
        "print(f\"Model 1 - Train Accuracy: {train_acc1:.4f}, Test Accuracy: {test_acc1:.4f}\")\n",
        "\n",
        "mlp_1_layer = MLP([784, 256, 10], [relu, softmax])\n",
        "mlp_2_layer = MLP([784, 256, 256, 10], [relu, relu, softmax])\n",
        "\n",
        "mlp_1_layer.fit(X_train, y_train_onehot, 64, 0.0001, 10)\n",
        "train_acc2 = mlp_1_layer.evaluate_acc(X_train, y_train)\n",
        "test_acc2 = mlp_1_layer.evaluate_acc(X_test, y_test)\n",
        "\n",
        "print(f\"Model 2 - Train Accuracy: {train_acc2:.4f}, Test Accuracy: {test_acc2:.4f}\")\n",
        "\n",
        "mlp_2_layer.fit(X_train, y_train_onehot, 64, 0.0001, 10)\n",
        "train_acc3 = mlp_2_layer.evaluate_acc(X_train, y_train)\n",
        "test_acc3 = mlp_2_layer.evaluate_acc(X_test, y_test)\n",
        "print(f\"Model 3 - Train Accuracy: {train_acc3:.4f}, Test Accuracy: {test_acc3:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing epoch 1...\n",
            "Processing epoch 1...\n",
            "New model with tanh - Test accuracy: 0.5598\n",
            "New model with leaky ReLU - Test accuracy: 0.8092\n"
          ]
        }
      ],
      "source": [
        "#3.2\n",
        "\n",
        "last_model_redo1 = MLP([784, 256, 256, 10], [tanh, tanh, softmax])\n",
        "last_model_redo2 = MLP([784, 256, 256, 10], [leaky_relu, leaky_relu, softmax])\n",
        "last_model_redo1.fit(X_train, y_train_onehot, 64, 0.001, 10)\n",
        "last_model_redo2.fit(X_train, y_train_onehot, 64, 0.001, 10)\n",
        "test_acc1 = last_model_redo1.evaluate_acc(X_test, y_test)\n",
        "test_acc2 = last_model_redo2.evaluate_acc(X_test, y_test)\n",
        "print(f\"New model with tanh - Test accuracy: {test_acc1:.4f}\")\n",
        "print(f\"New model with leaky ReLU - Test accuracy: {test_acc2:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3.2 Analysis:\n",
        "With a low learning rate (0.0001), the new model performed worse than the original (accuracy 0.589). This is likely because the gradients from tanh already cause a diminished shift in the weights when learning.\n",
        "We increase the learning rate to 0.001 to verify our assumption that learning rate should be increased when using a network with sigmoid or tanh, since their gradients are very small.\n",
        "\n",
        "Our intuition is confirmed -> with a higher learning rate of 0.001, our test accuracy jumps to 0.7294!\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing epoch 1...\n",
            "L1 - Test accuracy: 0.7700\n",
            "Processing epoch 1...\n",
            "L2 - Test accuracy: 0.7693\n"
          ]
        }
      ],
      "source": [
        "# 3.3\n",
        "\n",
        "model1 = MLP([784, 256, 256, 10], [relu, relu, softmax])\n",
        "model1.fit(X_train, y_train_onehot, 64, 0.0001, 10, l1_lambda=0.001)\n",
        "test_acc = model1.evaluate_acc(X_test, y_test)\n",
        "print(f\"L1 - Test accuracy: {test_acc:.4f}\")\n",
        "\n",
        "model2 = MLP([784, 256, 256, 10], [relu, relu, softmax])\n",
        "model2.fit(X_train, y_train_onehot, 64, 0.0001, 10, l2_lambda=0.001)\n",
        "test_acc = model2.evaluate_acc(X_test, y_test)\n",
        "print(f\"L2 - Test accuracy: {test_acc:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3.3 analysis\n",
        "With regularization, our model performed similarly to without. This is likely because our model does not overfit.\n",
        "If, on the other hand, our model performed better on the test set with regularization, that would indicate that our model without regularization overfitted to the training data.\n",
        "Conclusion: Regularization decreases model accuracy very slightly IF it does not overfit. However, if it does overfit, it can increase test accuracy greatly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing epoch 1...\n",
            "Unnormalized Model - Test Accuracy: 0.7476\n",
            "Normalized Model (from 3.1) - Test Accuracy: 0.7764\n"
          ]
        }
      ],
      "source": [
        "#3.4\n",
        "\n",
        "# 3.4 - Train on unnormalized images\n",
        "import torch\n",
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "\n",
        "def get_unnormalized_fashion_mnist_loaders(batch_size=128, val_ratio=0.2, seed=551):\n",
        "    \"\"\"\n",
        "    Load FashionMNIST without normalization for question 3.4\n",
        "    \"\"\"\n",
        "    # Simple transform that only converts to tensor (no normalization)\n",
        "    basic_transform = transforms.Compose([\n",
        "        transforms.ToTensor()  # Only converts to [0,1], no normalization\n",
        "    ])\n",
        "    \n",
        "    full_train_dataset = datasets.FashionMNIST(\n",
        "        root=\"./data\",\n",
        "        train=True,\n",
        "        download=True,\n",
        "        transform=basic_transform\n",
        "    )\n",
        "\n",
        "    test_dataset = datasets.FashionMNIST(\n",
        "        root=\"./data\",\n",
        "        train=False,\n",
        "        download=True,\n",
        "        transform=basic_transform\n",
        "    )\n",
        "\n",
        "    # Split train into train/val\n",
        "    total_train = len(full_train_dataset)\n",
        "    val_size = int(val_ratio * total_train)\n",
        "    train_size = total_train - val_size\n",
        "\n",
        "    generator = torch.Generator().manual_seed(seed)\n",
        "    train_dataset, val_dataset = random_split(\n",
        "        full_train_dataset,\n",
        "        [train_size, val_size],\n",
        "        generator=generator\n",
        "    )\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=2\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=2\n",
        "    )\n",
        "\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=2\n",
        "    )\n",
        "\n",
        "    return train_loader, val_loader, test_loader\n",
        "\n",
        "#unnormalized data\n",
        "train_loader_unnorm, val_loader_unnorm, test_loader_unnorm = get_unnormalized_fashion_mnist_loaders()\n",
        "\n",
        "# Convert to numpy arrays\n",
        "X_train_unnorm, y_train_unnorm = convert_loader_to_np(train_loader_unnorm)\n",
        "y_train_onehot_unnorm = labels_to_onehot(y_train_unnorm)\n",
        "\n",
        "X_test_unreg, y_test_unreg = convert_loader_to_np(test_loader_unnorm)\n",
        "\n",
        "# Train model on unnormalized data\n",
        "model_unnorm = MLP([784, 256, 256, 10], [relu, relu, softmax])\n",
        "model_unnorm.fit(X_train_unnorm, y_train_onehot_unnorm, 64, 0.0001, 10)\n",
        "\n",
        "# Evaluate\n",
        "train_acc_unnorm = model_unnorm.evaluate_acc(X_train_unnorm, y_train_unnorm)\n",
        "test_acc_unnorm = model_unnorm.evaluate_acc(X_test_unreg, y_test_unreg)\n",
        "\n",
        "print(f\"Unnormalized Model - Test Accuracy: {test_acc_unnorm:.4f}\")\n",
        "\n",
        "# Compare with normalized model from 3.1\n",
        "print(f\"Normalized Model (from 3.1) - Test Accuracy: {test_acc3:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3.4 Analysis:\n",
        "the unnormalized model performed worse than the normalized one. This is expected as normalization of data improves the dataset overall, which improves the training quality.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing epoch 1...\n",
            "Augmented Model - Test Accuracy: 0.6873\n",
            "Question 3 Model (from 3.3) - Test Accuracy: 0.7693\n",
            "3.5 MLP Training Time: 403.28s\n"
          ]
        }
      ],
      "source": [
        "# 3.5 - Data Augmentation\n",
        "\n",
        "import time\n",
        "\n",
        "def get_augmented_fashion_mnist_loaders(batch_size=128, val_ratio=0.2, seed=551, num_workers=0):\n",
        "    mean, std = compute_fashion_mnist_mean_std()\n",
        "\n",
        "    train_transform = transforms.Compose([\n",
        "        transforms.RandomHorizontalFlip(p=0.5),\n",
        "        transforms.RandomRotation(10),\n",
        "        transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((mean,), (std,))\n",
        "    ])\n",
        "\n",
        "    test_transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((mean,), (std,))\n",
        "    ])\n",
        "\n",
        "    # dataset only for splitting\n",
        "    base_train = datasets.FashionMNIST(\"./data\", train=True, download=True, transform=None)\n",
        "\n",
        "    total_train = len(base_train)\n",
        "    val_size = int(val_ratio * total_train)\n",
        "    train_size = total_train - val_size\n",
        "\n",
        "    generator = torch.Generator().manual_seed(seed)\n",
        "    train_indices, val_indices = random_split(range(total_train), [train_size, val_size], generator=generator)\n",
        "\n",
        "    # now create two datasets with different transforms\n",
        "    train_full = datasets.FashionMNIST(\"./data\", train=True, download=True, transform=train_transform)\n",
        "    val_full   = datasets.FashionMNIST(\"./data\", train=True, download=True, transform=test_transform)\n",
        "    test_full  = datasets.FashionMNIST(\"./data\", train=False, download=True, transform=test_transform)\n",
        "\n",
        "    train_dataset = Subset(train_full, train_indices.indices)\n",
        "    val_dataset   = Subset(val_full,   val_indices.indices)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,  num_workers=num_workers)\n",
        "    val_loader   = DataLoader(val_dataset,   batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
        "    test_loader  = DataLoader(test_full,     batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
        "\n",
        "    return train_loader, val_loader, test_loader\n",
        "\n",
        "\n",
        "# Load augmented data\n",
        "train_loader_aug, val_loader_aug, test_loader_aug = get_augmented_fashion_mnist_loaders()\n",
        "\n",
        "# Convert to numpy arrays\n",
        "X_train_aug, y_train_aug = convert_loader_to_np(train_loader_aug)\n",
        "y_train_onehot_aug = labels_to_onehot(y_train_aug)\n",
        "\n",
        "X_test_aug, y_test_aug = convert_loader_to_np(test_loader_aug)\n",
        "\n",
        "# Train model on augmented data (using same architecture as 3.1)\n",
        "model_aug = MLP([784, 256, 256, 10], [relu, relu, softmax])\n",
        "\n",
        "start_time_35 = time.time()\n",
        "model_aug.fit(X_train_aug, y_train_onehot_aug, 64, 0.0001, 10, l2_lambda=0.001)\n",
        "training_time_35 = time.time() - start_time_35\n",
        "\n",
        "# Evaluate\n",
        "test_acc_aug = model_aug.evaluate_acc(X_test_aug, y_test_aug)\n",
        "\n",
        "print(f\"Augmented Model - Test Accuracy: {test_acc_aug:.4f}\")\n",
        "print(f\"Question 3 Model (from 3.3) - Test Accuracy: {test_acc:.4f}\")\n",
        "print(f\"3.5 MLP Training Time: {training_time_35:.2f}s\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3.5: BROKEN, AUGMENTED DATA DOES WORSE??"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training CNN...\n",
            "Epoch 1/10, Loss: 0.5408, Val Accuracy: 87.41%\n",
            "Epoch 2/10, Loss: 0.3484, Val Accuracy: 89.17%\n",
            "Epoch 3/10, Loss: 0.2941, Val Accuracy: 90.12%\n",
            "Epoch 4/10, Loss: 0.2646, Val Accuracy: 91.35%\n",
            "Epoch 5/10, Loss: 0.2401, Val Accuracy: 91.71%\n",
            "Epoch 6/10, Loss: 0.2163, Val Accuracy: 92.08%\n",
            "Epoch 7/10, Loss: 0.2010, Val Accuracy: 92.40%\n",
            "Epoch 8/10, Loss: 0.1835, Val Accuracy: 92.04%\n",
            "Epoch 9/10, Loss: 0.1703, Val Accuracy: 92.08%\n",
            "Epoch 10/10, Loss: 0.1555, Val Accuracy: 92.51%\n",
            "CNN Test Accuracy: 91.96%\n",
            "Best MLP Test Accuracy (from 3.1): 0.7764 (77.64%)\n",
            "CNN Test Accuracy: 91.96%\n",
            "CNN improved accuracy by 18.44% compared to MLP\n"
          ]
        }
      ],
      "source": [
        "# 3.6 - CNN Implementation with PyTorch\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class FashionMNISTCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FashionMNISTCNN, self).__init__()\n",
        "        #layer1\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)  #1 in, 32out\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1) #32 in, 64 out\n",
        "        \n",
        "        #full connected layer\n",
        "        self.fc1 = nn.Linear(64 * 7 * 7, 256)  #after 2 maxpool layers: 28x28 -> 14x14 -> 7x7\n",
        "        self.fc2 = nn.Linear(256, 10)  #output layer for 10 classes\n",
        "        \n",
        "        #introduce dropout\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        #first conv block: Conv -> ReLU -> MaxPool\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.max_pool2d(x, 2)  # 28x28 -> 14x14 (downsample: https://docs.pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html)\n",
        "        \n",
        "        #second conv block: Conv -> ReLU -> MaxPool\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.max_pool2d(x, 2)  # 14x14 -> 7x7\n",
        "        \n",
        "        #flatten fully connected layers\n",
        "        x = x.view(-1, 64 * 7 * 7)\n",
        "        \n",
        "        # Fully connected layers with ReLU\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        \n",
        "        return x\n",
        "\n",
        "    def train_model(self, train_loader, val_loader, epochs=10, lr=0.001):\n",
        "        \"\"\"\n",
        "        Train the CNN model\n",
        "        \"\"\"\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.to(device)\n",
        "        \n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = optim.Adam(self.parameters(), lr=lr)\n",
        "        \n",
        "        train_losses = []\n",
        "        val_accuracies = []\n",
        "        \n",
        "        for epoch in range(epochs):\n",
        "            #nn.Module train() method\n",
        "            self.train()  #sets model to training mode (enables dropout, etc.)\n",
        "            running_loss = 0.0\n",
        "            \n",
        "            for batch_idx, (data, target) in enumerate(train_loader):\n",
        "                data, target = data.to(device), target.to(device)\n",
        "                \n",
        "                optimizer.zero_grad()\n",
        "                output = self(data)\n",
        "                loss = criterion(output, target)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                \n",
        "                running_loss += loss.item()\n",
        "            \n",
        "            # validation phase\n",
        "            self.eval()  #set model to evaluation mode (disables dropout, etc.)\n",
        "            correct = 0\n",
        "            total = 0\n",
        "            \n",
        "            with torch.no_grad():\n",
        "                for data, target in val_loader:\n",
        "                    data, target = data.to(device), target.to(device)\n",
        "                    output = self(data)\n",
        "                    _, predicted = torch.max(output.data, 1)\n",
        "                    total += target.size(0)\n",
        "                    correct += (predicted == target).sum().item()\n",
        "            \n",
        "            val_accuracy = 100 * correct / total\n",
        "            avg_loss = running_loss / len(train_loader)\n",
        "            \n",
        "            train_losses.append(avg_loss)\n",
        "            val_accuracies.append(val_accuracy)\n",
        "            \n",
        "            print(f'Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%')\n",
        "        \n",
        "        return train_losses, val_accuracies\n",
        "\n",
        "    def evaluate(self, test_loader):\n",
        "        \"\"\"\n",
        "        Evaluate the CNN model on test set\n",
        "        \"\"\"\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.to(device)\n",
        "        \n",
        "        self.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            for data, target in test_loader:\n",
        "                data, target = data.to(device), target.to(device)\n",
        "                output = self(data)\n",
        "                _, predicted = torch.max(output.data, 1)\n",
        "                total += target.size(0)\n",
        "                correct += (predicted == target).sum().item()\n",
        "        \n",
        "        accuracy = 100 * correct / total\n",
        "        return accuracy\n",
        "\n",
        "#data loaders\n",
        "train_loader, val_loader, test_loader, mean, std = get_fashion_mnist_loaders(batch_size=128)\n",
        "\n",
        "#create/train cnn\n",
        "cnn_model = FashionMNISTCNN()\n",
        "print(\"Training CNN...\")\n",
        "train_losses, val_accuracies = cnn_model.train_model(train_loader, val_loader, epochs=10)\n",
        "\n",
        "#evaluate\n",
        "test_accuracy = cnn_model.evaluate(test_loader)\n",
        "print(f\"CNN Test Accuracy: {test_accuracy:.2f}%\")\n",
        "\n",
        "#compare with mlp\n",
        "print(f\"Best MLP Test Accuracy (from 3.1): {test_acc3:.4f} ({test_acc3*100:.2f}%)\")\n",
        "print(f\"CNN Test Accuracy: {test_accuracy:.2f}%\")\n",
        "\n",
        "#analysis\n",
        "if test_accuracy/100 > test_acc3:\n",
        "    improvement = ((test_accuracy/100 - test_acc3) / test_acc3) * 100\n",
        "    print(f\"CNN improved accuracy by {improvement:.2f}% compared to MLP\")\n",
        "else:\n",
        "    difference = ((test_acc3 - test_accuracy/100) / test_acc3) * 100\n",
        "    print(f\"MLP was {difference:.2f}% better than CNN\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "no augmentation\n",
            "Epoch 1/10, Time: 13.88s, Loss: 0.5270, Val Accuracy: 88.08%\n",
            "Epoch 2/10, Time: 12.50s, Loss: 0.3355, Val Accuracy: 89.77%\n",
            "Epoch 3/10, Time: 12.63s, Loss: 0.2870, Val Accuracy: 90.07%\n",
            "Epoch 4/10, Time: 12.82s, Loss: 0.2564, Val Accuracy: 91.27%\n",
            "Epoch 5/10, Time: 12.54s, Loss: 0.2313, Val Accuracy: 91.95%\n",
            "Epoch 6/10, Time: 13.10s, Loss: 0.2113, Val Accuracy: 91.85%\n",
            "Epoch 7/10, Time: 12.70s, Loss: 0.1952, Val Accuracy: 91.78%\n",
            "Epoch 8/10, Time: 12.49s, Loss: 0.1770, Val Accuracy: 92.17%\n",
            "Epoch 9/10, Time: 12.43s, Loss: 0.1655, Val Accuracy: 92.19%\n",
            "Epoch 10/10, Time: 12.38s, Loss: 0.1499, Val Accuracy: 92.62%\n",
            "Regular CNN - Test Accuracy: 92.19%\n",
            "Regular CNN - Training Time: 127.47s\n",
            "Epoch 1/10, Time: 12.61s, Loss: 0.7838, Val Accuracy: 82.41%\n",
            "Epoch 2/10, Time: 13.51s, Loss: 0.5492, Val Accuracy: 84.81%\n",
            "Epoch 3/10, Time: 11.99s, Loss: 0.4810, Val Accuracy: 86.96%\n",
            "Epoch 4/10, Time: 11.98s, Loss: 0.4397, Val Accuracy: 88.20%\n",
            "Epoch 5/10, Time: 11.80s, Loss: 0.4169, Val Accuracy: 87.92%\n",
            "Epoch 6/10, Time: 12.24s, Loss: 0.3940, Val Accuracy: 89.36%\n",
            "Epoch 7/10, Time: 11.91s, Loss: 0.3788, Val Accuracy: 89.56%\n",
            "Epoch 8/10, Time: 12.72s, Loss: 0.3663, Val Accuracy: 89.53%\n",
            "Epoch 9/10, Time: 13.89s, Loss: 0.3626, Val Accuracy: 89.68%\n",
            "Epoch 10/10, Time: 11.60s, Loss: 0.3472, Val Accuracy: 90.40%\n",
            "Augmented CNN: Test Accuracy: 90.33%\n",
            "Augmented CNN: Training Time: 124.26s\n",
            "\n",
            "PERFORMANCE COMPARISON\n",
            "Regular CNN - Test Accuracy: 92.19%\n",
            "Augmented CNN - Test Accuracy: 90.33%\n",
            "Regular CNN - Training Time: 127.47s\n",
            "Augmented CNN - Training Time: 124.26s\n",
            "\n",
            "Accuracy Difference: -1.86%\n",
            "Training Time Difference: -3.21s\n",
            "Data augmentation REDUCED accuracy\n",
            "Data augmentation DECREASED training time\n",
            "\n",
            "ADDITIONAL ANALYSIS\n",
            "Regular CNN Final Validation Accuracies:\n",
            "  Epoch 9: 92.17%\n",
            "  Epoch 10: 92.19%\n",
            "  Epoch 11: 92.62%\n",
            "Augmented CNN Final Validation Accuracies:\n",
            "  Epoch 9: 89.53%\n",
            "  Epoch 10: 89.68%\n",
            "  Epoch 11: 90.40%\n"
          ]
        }
      ],
      "source": [
        "#3.7: Train CNN with Data Augmentation\n",
        "\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import random_split\n",
        "\n",
        "class FashionMNISTCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FashionMNISTCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.fc1 = nn.Linear(64 * 7 * 7, 256)\n",
        "        self.fc2 = nn.Linear(256, 10)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = x.view(-1, 64 * 7 * 7)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "    def train_model(self, train_loader, val_loader, epochs=10, lr=0.001):\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.to(device)\n",
        "        \n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = optim.Adam(self.parameters(), lr=lr)\n",
        "        \n",
        "        train_losses = []\n",
        "        val_accuracies = []\n",
        "        \n",
        "        for epoch in range(epochs):\n",
        "            start_time = time.time()\n",
        "            \n",
        "            #training phase\n",
        "            self.train()\n",
        "            running_loss = 0.0\n",
        "            \n",
        "            for batch_idx, (data, target) in enumerate(train_loader):\n",
        "                data, target = data.to(device), target.to(device)\n",
        "                \n",
        "                optimizer.zero_grad()\n",
        "                output = self(data)\n",
        "                loss = criterion(output, target)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                \n",
        "                running_loss += loss.item()\n",
        "            \n",
        "            #validation phase\n",
        "            self.eval()\n",
        "            correct = 0\n",
        "            total = 0\n",
        "            \n",
        "            with torch.no_grad():\n",
        "                for data, target in val_loader:\n",
        "                    data, target = data.to(device), target.to(device)\n",
        "                    output = self(data)\n",
        "                    _, predicted = torch.max(output.data, 1)\n",
        "                    total += target.size(0)\n",
        "                    correct += (predicted == target).sum().item()\n",
        "            \n",
        "            epoch_time = time.time() - start_time\n",
        "            val_accuracy = 100 * correct / total\n",
        "            avg_loss = running_loss / len(train_loader)\n",
        "            \n",
        "            train_losses.append(avg_loss)\n",
        "            val_accuracies.append(val_accuracy)\n",
        "            \n",
        "            print(f'Epoch {epoch+1}/{epochs}, Time: {epoch_time:.2f}s, Loss: {avg_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%')\n",
        "        \n",
        "        return train_losses, val_accuracies\n",
        "\n",
        "    def evaluate(self, test_loader):\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.to(device)\n",
        "        \n",
        "        self.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            for data, target in test_loader:\n",
        "                data, target = data.to(device), target.to(device)\n",
        "                output = self(data)\n",
        "                _, predicted = torch.max(output.data, 1)\n",
        "                total += target.size(0)\n",
        "                correct += (predicted == target).sum().item()\n",
        "        \n",
        "        accuracy = 100 * correct / total\n",
        "        return accuracy\n",
        "\n",
        "def get_augmented_fashion_mnist_loaders(batch_size=128, val_ratio=0.2, seed=551, num_workers=0):\n",
        "    mean, std = compute_fashion_mnist_mean_std()\n",
        "\n",
        "    train_transform = transforms.Compose([\n",
        "        transforms.RandomHorizontalFlip(p=0.5),\n",
        "        transforms.RandomRotation(10),\n",
        "        transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((mean,), (std,))\n",
        "    ])\n",
        "\n",
        "    test_transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((mean,), (std,))\n",
        "    ])\n",
        "\n",
        "    #dataset only for splitting\n",
        "    base_train = datasets.FashionMNIST(\"./data\", train=True, download=True, transform=None)\n",
        "\n",
        "    total_train = len(base_train)\n",
        "    val_size = int(val_ratio * total_train)\n",
        "    train_size = total_train - val_size\n",
        "\n",
        "    generator = torch.Generator().manual_seed(seed)\n",
        "    train_indices, val_indices = random_split(range(total_train), [train_size, val_size], generator=generator)\n",
        "\n",
        "    #now create two datasets with different transforms\n",
        "    train_full = datasets.FashionMNIST(\"./data\", train=True, download=True, transform=train_transform)\n",
        "    val_full   = datasets.FashionMNIST(\"./data\", train=True, download=True, transform=test_transform)\n",
        "    test_full  = datasets.FashionMNIST(\"./data\", train=False, download=True, transform=test_transform)\n",
        "\n",
        "    train_dataset = Subset(train_full, train_indices.indices)\n",
        "    val_dataset   = Subset(val_full,   val_indices.indices)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,  num_workers=num_workers)\n",
        "    val_loader   = DataLoader(val_dataset,   batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
        "    test_loader  = DataLoader(test_full,     batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
        "\n",
        "    return train_loader, val_loader, test_loader\n",
        "\n",
        "#without augmentation\n",
        "print(\"no augmentation\")\n",
        "train_loader_regular, val_loader_regular, test_loader_regular, _, _ = get_fashion_mnist_loaders(batch_size=128)\n",
        "\n",
        "cnn_regular = FashionMNISTCNN()\n",
        "start_time_regular = time.time()\n",
        "train_losses_regular, val_accuracies_regular = cnn_regular.train_model(\n",
        "    train_loader_regular, val_loader_regular, epochs=10\n",
        ")\n",
        "training_time_regular = time.time() - start_time_regular\n",
        "\n",
        "test_accuracy_regular = cnn_regular.evaluate(test_loader_regular)\n",
        "print(f\"Regular CNN - Test Accuracy: {test_accuracy_regular:.2f}%\")\n",
        "print(f\"Regular CNN - Training Time: {training_time_regular:.2f}s\")\n",
        "\n",
        "# with augmentation\n",
        "train_loader_aug, val_loader_aug, test_loader_aug = get_augmented_fashion_mnist_loaders(batch_size=128)\n",
        "\n",
        "cnn_augmented = FashionMNISTCNN()\n",
        "start_time_aug = time.time()\n",
        "train_losses_aug, val_accuracies_aug = cnn_augmented.train_model(\n",
        "    train_loader_aug, val_loader_aug, epochs=10\n",
        ")\n",
        "training_time_aug = time.time() - start_time_aug\n",
        "\n",
        "test_accuracy_aug = cnn_augmented.evaluate(test_loader_aug)\n",
        "print(f\"Augmented CNN: Test Accuracy: {test_accuracy_aug:.2f}%\")\n",
        "print(f\"Augmented CNN: Training Time: {training_time_aug:.2f}s\")\n",
        "\n",
        "# Performance Comparison\n",
        "print(\"\\nPERFORMANCE COMPARISON\")\n",
        "print(f\"Regular CNN - Test Accuracy: {test_accuracy_regular:.2f}%\")\n",
        "print(f\"Augmented CNN - Test Accuracy: {test_accuracy_aug:.2f}%\")\n",
        "print(f\"Regular CNN - Training Time: {training_time_regular:.2f}s\")\n",
        "print(f\"Augmented CNN - Training Time: {training_time_aug:.2f}s\")\n",
        "\n",
        "accuracy_difference = test_accuracy_aug - test_accuracy_regular\n",
        "time_difference = training_time_aug - training_time_regular\n",
        "\n",
        "print(f\"\\nAccuracy Difference: {accuracy_difference:+.2f}%\")\n",
        "print(f\"Training Time Difference: {time_difference:+.2f}s\")\n",
        "\n",
        "if accuracy_difference > 0:\n",
        "    print(\"Data augmentation IMPROVED accuracy\")\n",
        "else:\n",
        "    print(\"Data augmentation REDUCED accuracy\")\n",
        "\n",
        "if time_difference > 0:\n",
        "    print(\"Data augmentation INCREASED training time\")\n",
        "else:\n",
        "    print(\"Data augmentation DECREASED training time\")\n",
        "\n",
        "#additional analysis\n",
        "print(\"\\nADDITIONAL ANALYSIS\")\n",
        "print(\"Regular CNN Final Validation Accuracies:\")\n",
        "for i, acc in enumerate(val_accuracies_regular[-3:], len(val_accuracies_regular)-2):\n",
        "    print(f\"  Epoch {i+1}: {acc:.2f}%\")\n",
        "\n",
        "print(\"Augmented CNN Final Validation Accuracies:\")\n",
        "for i, acc in enumerate(val_accuracies_aug[-3:], len(val_accuracies_aug)-2):\n",
        "    print(f\"  Epoch {i+1}: {acc:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training pretrained ResNet18 with head configuration: fc_512_10, layers: []\n",
            "Epoch 1/10, Loss: 1.3405, Train Acc: 53.73%, Val Acc: 64.08%\n",
            "Epoch 2/10, Loss: 1.1236, Train Acc: 60.74%, Val Acc: 66.34%\n",
            "Epoch 3/10, Loss: 1.0827, Train Acc: 62.04%, Val Acc: 65.58%\n",
            "Epoch 4/10, Loss: 1.0599, Train Acc: 62.92%, Val Acc: 67.11%\n",
            "Epoch 5/10, Loss: 1.0574, Train Acc: 62.74%, Val Acc: 67.76%\n",
            "Epoch 6/10, Loss: 1.0434, Train Acc: 63.09%, Val Acc: 67.59%\n",
            "Epoch 7/10, Loss: 1.0416, Train Acc: 63.35%, Val Acc: 66.78%\n",
            "Epoch 8/10, Loss: 1.0316, Train Acc: 63.83%, Val Acc: 68.26%\n",
            "Epoch 9/10, Loss: 1.0346, Train Acc: 63.23%, Val Acc: 68.16%\n",
            "Epoch 10/10, Loss: 1.0418, Train Acc: 63.19%, Val Acc: 68.05%\n",
            "Head fc_512_10 - Best Val Acc: 68.26%, Test Acc: 67.65%, Training Time: 138.74s\n",
            "\n",
            "Training pretrained ResNet18 with head configuration: fc_512_256_10, layers: [256]\n",
            "Epoch 1/10, Loss: 1.3268, Train Acc: 52.70%, Val Acc: 65.40%\n",
            "Epoch 2/10, Loss: 1.1304, Train Acc: 59.02%, Val Acc: 67.28%\n",
            "Epoch 3/10, Loss: 1.0769, Train Acc: 60.74%, Val Acc: 67.49%\n",
            "Epoch 4/10, Loss: 1.0676, Train Acc: 61.25%, Val Acc: 68.47%\n",
            "Epoch 5/10, Loss: 1.0511, Train Acc: 62.03%, Val Acc: 68.97%\n",
            "Epoch 6/10, Loss: 1.0413, Train Acc: 62.45%, Val Acc: 69.43%\n",
            "Epoch 7/10, Loss: 1.0324, Train Acc: 62.52%, Val Acc: 69.95%\n",
            "Epoch 8/10, Loss: 1.0257, Train Acc: 62.83%, Val Acc: 69.81%\n",
            "Epoch 9/10, Loss: 1.0154, Train Acc: 63.14%, Val Acc: 69.89%\n",
            "Epoch 10/10, Loss: 1.0135, Train Acc: 63.11%, Val Acc: 70.74%\n",
            "Head fc_512_256_10 - Best Val Acc: 70.74%, Test Acc: 70.18%, Training Time: 133.10s\n",
            "\n",
            "Training pretrained ResNet18 with head configuration: fc_512_256_256_10, layers: [256, 256]\n",
            "Epoch 1/10, Loss: 1.4155, Train Acc: 48.72%, Val Acc: 65.43%\n",
            "Epoch 2/10, Loss: 1.1840, Train Acc: 57.37%, Val Acc: 66.42%\n",
            "Epoch 3/10, Loss: 1.1348, Train Acc: 58.94%, Val Acc: 66.45%\n",
            "Epoch 4/10, Loss: 1.1140, Train Acc: 59.90%, Val Acc: 68.83%\n",
            "Epoch 5/10, Loss: 1.0937, Train Acc: 60.67%, Val Acc: 68.41%\n",
            "Epoch 6/10, Loss: 1.0744, Train Acc: 61.24%, Val Acc: 69.33%\n",
            "Epoch 7/10, Loss: 1.0713, Train Acc: 61.58%, Val Acc: 68.09%\n",
            "Epoch 8/10, Loss: 1.0689, Train Acc: 61.52%, Val Acc: 69.31%\n",
            "Epoch 9/10, Loss: 1.0671, Train Acc: 61.89%, Val Acc: 69.47%\n",
            "Epoch 10/10, Loss: 1.0509, Train Acc: 61.96%, Val Acc: 69.88%\n",
            "Head fc_512_256_256_10 - Best Val Acc: 69.88%, Test Acc: 69.36%, Training Time: 138.48s\n",
            "\n",
            "Best pretrained head configuration (by validation accuracy):\n",
            "fc_512_256_10 {'fc_layers': [256], 'best_val_acc': 70.74166666666666, 'test_acc': 70.18, 'train_time': 133.09520387649536}\n",
            "\n",
            "Best pretrained model (3.8) - Test Accuracy: 70.18%\n",
            "Best pretrained model (3.8) - Training Time: 133.10s\n",
            "\n",
            "Best MLP with augmentation (3.5) - Test Accuracy: 68.73%\n",
            "Best MLP with augmentation (3.5) - Training Time: 403.28s\n",
            "\n",
            "CNN with augmentation (3.7) - Test Accuracy: 90.33%\n",
            "CNN with augmentation (3.7) - Training Time: 124.26s\n"
          ]
        }
      ],
      "source": [
        "# 3.8 - Pretrained CNN with frozen convolutional layers and trainable fully connected head\n",
        "\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import models, datasets, transforms\n",
        "from torch.utils.data import DataLoader, random_split, Subset\n",
        "\n",
        "class PretrainedResNetFashion(nn.Module):\n",
        "    def __init__(self, fc_hidden_layers=None, num_classes=10):\n",
        "        \"\"\"\n",
        "        Wrap a ResNet18 pretrained on ImageNet.\n",
        "        Freeze convolutional layers and replace the final fully connected layer\n",
        "        with a new head defined by fc_hidden_layers.\n",
        "\n",
        "        Examples for fc_hidden_layers:\n",
        "          []          -> 512 -> num_classes\n",
        "          [256]       -> 512 -> 256 -> num_classes\n",
        "          [256, 256]  -> 512 -> 256 -> 256 -> num_classes\n",
        "        \"\"\"\n",
        "        super(PretrainedResNetFashion, self).__init__()\n",
        "        if fc_hidden_layers is None:\n",
        "            fc_hidden_layers = []\n",
        "\n",
        "        # Load pretrained ResNet18 (handle both newer and older torchvision APIs)\n",
        "        try:\n",
        "            resnet = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
        "        except AttributeError:\n",
        "            resnet = models.resnet18(pretrained=True)\n",
        "\n",
        "        # Freeze all pretrained parameters (convolutional backbone)\n",
        "        for param in resnet.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        in_features = resnet.fc.in_features\n",
        "\n",
        "        head_layers = []\n",
        "        prev_dim = in_features\n",
        "        for hidden_dim in fc_hidden_layers:\n",
        "            head_layers.append(nn.Linear(prev_dim, hidden_dim))\n",
        "            head_layers.append(nn.ReLU())\n",
        "            head_layers.append(nn.Dropout(0.5))\n",
        "            prev_dim = hidden_dim\n",
        "        head_layers.append(nn.Linear(prev_dim, num_classes))\n",
        "\n",
        "        resnet.fc = nn.Sequential(*head_layers)\n",
        "        self.model = resnet\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "\n",
        "def get_pretrained_aug_fashion_mnist_loaders(batch_size=128, val_ratio=0.2, seed=551, num_workers=0):\n",
        "    \"\"\"\n",
        "    Data loaders for pretrained ResNet on FashionMNIST with data augmentation\n",
        "    consistent with question 3.5.\n",
        "\n",
        "    - Augmentation (train only): random horizontal flip, rotation, translation\n",
        "    - Convert 1-channel FashionMNIST images to 3-channel by duplication\n",
        "    - Normalize with dataset mean/std expanded to 3 channels\n",
        "    \"\"\"\n",
        "    mean, std = compute_fashion_mnist_mean_std()\n",
        "    mean_3ch = (mean, mean, mean)\n",
        "    std_3ch = (std, std, std)\n",
        "\n",
        "    train_transform = transforms.Compose([\n",
        "        transforms.RandomHorizontalFlip(p=0.5),\n",
        "        transforms.RandomRotation(10),\n",
        "        transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Lambda(lambda t: t.expand(3, -1, -1)),\n",
        "        transforms.Normalize(mean_3ch, std_3ch)\n",
        "    ])\n",
        "\n",
        "    test_transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Lambda(lambda t: t.expand(3, -1, -1)),\n",
        "        transforms.Normalize(mean_3ch, std_3ch)\n",
        "    ])\n",
        "\n",
        "    # Base dataset only for defining a fixed train/val split\n",
        "    base_train = datasets.FashionMNIST(\"./data\", train=True, download=True, transform=None)\n",
        "\n",
        "    total_train = len(base_train)\n",
        "    val_size = int(val_ratio * total_train)\n",
        "    train_size = total_train - val_size\n",
        "\n",
        "    generator = torch.Generator().manual_seed(seed)\n",
        "    train_indices, val_indices = random_split(range(total_train), [train_size, val_size], generator=generator)\n",
        "\n",
        "    # Datasets with actual transforms\n",
        "    train_full = datasets.FashionMNIST(\"./data\", train=True, download=True, transform=train_transform)\n",
        "    val_full   = datasets.FashionMNIST(\"./data\", train=True, download=True, transform=test_transform)\n",
        "    test_full  = datasets.FashionMNIST(\"./data\", train=False, download=True, transform=test_transform)\n",
        "\n",
        "    train_dataset = Subset(train_full, train_indices.indices)\n",
        "    val_dataset   = Subset(val_full,   val_indices.indices)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,  num_workers=num_workers)\n",
        "    val_loader   = DataLoader(val_dataset,   batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
        "    test_loader  = DataLoader(test_full,     batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
        "\n",
        "    return train_loader, val_loader, test_loader\n",
        "\n",
        "\n",
        "def train_pretrained_model(model, train_loader, val_loader, epochs=5, lr=1e-3):\n",
        "    \"\"\"\n",
        "    Train only the fully connected head of a pretrained model.\n",
        "    \"\"\"\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.to(device)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n",
        "\n",
        "    train_losses = []\n",
        "    val_accuracies = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        avg_loss = running_loss / len(train_loader)\n",
        "        train_acc = 100.0 * correct / total\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for images, labels in val_loader:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                outputs = model(images)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "        val_acc = 100.0 * correct / total\n",
        "\n",
        "        train_losses.append(avg_loss)\n",
        "        val_accuracies.append(val_acc)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}, \"\n",
        "              f\"Train Acc: {train_acc:.2f}%, Val Acc: {val_acc:.2f}%\")\n",
        "\n",
        "    return train_losses, val_accuracies\n",
        "\n",
        "\n",
        "def evaluate_pretrained(model, test_loader):\n",
        "    \"\"\"\n",
        "    Evaluate pretrained model on the test set and return accuracy in percent.\n",
        "    \"\"\"\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100.0 * correct / total\n",
        "    return accuracy\n",
        "\n",
        "\n",
        "# Run experiments with different FC head depths to justify the choice\n",
        "\n",
        "train_loader_pre, val_loader_pre, test_loader_pre = get_pretrained_aug_fashion_mnist_loaders(batch_size=128)\n",
        "\n",
        "fc_head_configs = {\n",
        "    \"fc_512_10\": [],\n",
        "    \"fc_512_256_10\": [256],\n",
        "    \"fc_512_256_256_10\": [256, 256]\n",
        "}\n",
        "\n",
        "pretrained_results = {}\n",
        "\n",
        "for name, fc_layers in fc_head_configs.items():\n",
        "    print(f\"\\nTraining pretrained ResNet18 with head configuration: {name}, layers: {fc_layers}\")\n",
        "    model_pre = PretrainedResNetFashion(fc_hidden_layers=fc_layers, num_classes=10)\n",
        "\n",
        "    start_time = time.time()\n",
        "    train_losses_pre, val_accs_pre = train_pretrained_model(\n",
        "        model_pre, train_loader_pre, val_loader_pre, epochs=10, lr=1e-3\n",
        "    )\n",
        "    train_time = time.time() - start_time\n",
        "\n",
        "    test_acc_pre = evaluate_pretrained(model_pre, test_loader_pre)\n",
        "    best_val_acc = max(val_accs_pre) if len(val_accs_pre) > 0 else 0.0\n",
        "\n",
        "    pretrained_results[name] = {\n",
        "        \"fc_layers\": fc_layers,\n",
        "        \"best_val_acc\": best_val_acc,\n",
        "        \"test_acc\": test_acc_pre,\n",
        "        \"train_time\": train_time\n",
        "    }\n",
        "\n",
        "    print(f\"Head {name} - Best Val Acc: {best_val_acc:.2f}%, \"\n",
        "          f\"Test Acc: {test_acc_pre:.2f}%, Training Time: {train_time:.2f}s\")\n",
        "\n",
        "# Select best head by validation accuracy\n",
        "best_head_name = max(pretrained_results, key=lambda k: pretrained_results[k][\"best_val_acc\"])\n",
        "best_head_info = pretrained_results[best_head_name]\n",
        "\n",
        "print(\"\\nBest pretrained head configuration (by validation accuracy):\")\n",
        "print(best_head_name, best_head_info)\n",
        "\n",
        "print(f\"\\nBest pretrained model (3.8) - Test Accuracy: {best_head_info['test_acc']:.2f}%\")\n",
        "print(f\"Best pretrained model (3.8) - Training Time: {best_head_info['train_time']:.2f}s\")\n",
        "\n",
        "# 3.5 MLP WITH AUGMENTATION\n",
        "if 'test_acc_aug' in globals() and 'training_time_35' in globals():\n",
        "    print(f\"\\nBest MLP with augmentation (3.5) - Test Accuracy: {test_acc_aug * 100:.2f}%\")\n",
        "    print(f\"Best MLP with augmentation (3.5) - Training Time: {training_time_35:.2f}s\")\n",
        "\n",
        "# 3.7 CNN WITH AUGMENTATION\n",
        "if 'test_accuracy_aug' in globals() and 'training_time_aug' in globals():\n",
        "    print(f\"\\nCNN with augmentation (3.7) - Test Accuracy: {test_accuracy_aug:.2f}%\")\n",
        "    print(f\"CNN with augmentation (3.7) - Training Time: {training_time_aug:.2f}s\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
