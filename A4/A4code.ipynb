{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/bin/python3\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "mount failed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-629139760.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipython-input-629139760.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;31m# 1. Load raw data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m     \u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_main\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_sub\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_wos_txt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Loaded {len(texts)} abstracts.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipython-input-629139760.py\u001b[0m in \u001b[0;36mload_wos_txt\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;31m# this will go into your google drive/dataset/ to get the files, so upload the files in there if you need to run this\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m     \u001b[0mx_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"./dataset/X.txt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0myl1_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"./dataset/YL1.txt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m     98\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    270\u001b[0m             \u001b[0;34m'https://research.google.com/colaboratory/faq.html#drive-timeout'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m         )\n\u001b[0;32m--> 272\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mount failed'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mextra_reason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m       \u001b[0;31m# Terminate the DriveFS binary before killing bash.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: mount failed"
     ]
    }
   ],
   "source": [
    "# Task 1, Preparing the dataset\n",
    "# WOS-11967 is a list of 11967 abstracts from scientific papers alongside 2 labels for each instance - (main domain, sub domain)\n",
    "\n",
    "\"\"\"\n",
    "1. Load the raw text and labels. (each abstract is on each line)\n",
    "2. Tokenize the abstracts.\n",
    "3. Build a vocabulary for word-level LSTM. (LSTM needs words, not tokens like BERT)\n",
    "4. Encode each abstract as a padded sequence of word IDs.\n",
    "5. Track the true sequence lengths (before padding).\n",
    "6. Split into train/val/test sets (categorized by main label).\n",
    "7. Save processed tensors to disk for later use.\n",
    "\n",
    "outputs:\n",
    "- wos_lstm_train.pt\n",
    "- wos_lstm_val.pt\n",
    "- wos_lstm_test.pt\n",
    "- wos_bert_train.pt\n",
    "- wos_bert_val.pt\n",
    "- wos_bert_test.pt\n",
    "\"\"\"\n",
    "\n",
    "# NOTE: THE CODE WAS RUN ON GOOGLE COLAB SEPARATELY.\n",
    "\n",
    "import os\n",
    "import re\n",
    "import argparse\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# load raw dataset files\n",
    "\"\"\"\n",
    "returns\n",
    "- texts: list[abstract]\n",
    "- labels_main: np.array(11967)\n",
    "- labels_sub: np.array(11967)\n",
    "\"\"\"\n",
    "\n",
    "def load_wos_txt():\n",
    "    x_path = \"./dataset/X.txt\"\n",
    "    yl1_path = \"./dataset/YL1.txt\"\n",
    "    yl2_path = \"./dataset/YL2.txt\"\n",
    "\n",
    "    with open(x_path, encoding=\"utf-8\") as f:\n",
    "        texts = [line.strip() for line in f]\n",
    "\n",
    "    with open(yl1_path, encoding=\"utf-8\") as f:\n",
    "        labels_main = [int(line.strip()) for line in f]\n",
    "\n",
    "    with open(yl2_path, encoding=\"utf-8\") as f:\n",
    "        labels_sub = [int(line.strip()) for line in f]\n",
    "\n",
    "    return texts, np.array(labels_main, dtype=np.int64), np.array(labels_sub, dtype=np.int64)\n",
    "\n",
    "\n",
    "# tokenization, regex is to replace any non \"regular\" characters with spaces\n",
    "_token_pattern = re.compile(r\"[^a-z0-9]+\")\n",
    "\n",
    "def simple_tokenize(text):\n",
    "    text = text.lower()\n",
    "    text = _token_pattern.sub(\" \", text)\n",
    "    return text.split()\n",
    "\n",
    "def tokenize_inputs(texts):\n",
    "    return [simple_tokenize(t) for t in texts]\n",
    "\n",
    "# super simple \"embedding\" method; go over all the words, and as long as they appear more than twice (prevent URLs and stuff like that),\n",
    "# add to the vocab with id = len(vocab) (which means each new vocab word added, the length of vocab increases)\n",
    "# keep <PAD> and <UNK> for padding (PAD in case abstract too short, UNK for \"unknown\" token in case an unknown word appears) \n",
    "def build_vocab(tokenized_texts, min_freq):\n",
    "    counter = Counter()\n",
    "    for toks in tokenized_texts:\n",
    "        counter.update(toks)\n",
    "\n",
    "    vocab = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
    "    for tok, freq in counter.items():\n",
    "        if freq >= min_freq:\n",
    "            vocab[tok] = len(vocab)\n",
    "    return vocab\n",
    "\n",
    "\"\"\"\n",
    "output is\n",
    "- input_ids: N x max_len matrix of our embeddings. Contains our custom embedding logic + PAD whenever the abstract length < max_len.\n",
    "    obviously, if abstract length > max_len, it gets right truncated\n",
    "- lengths: N length array of (actual) lengths of the abstracts. Of course, if ith abstract length > max_len, lengths[i] = max_len \n",
    "\"\"\"\n",
    "def encode_and_pad(tokenized_texts, vocab, max_len=None):\n",
    "    lengths_raw = np.array([len(toks) for toks in tokenized_texts], dtype=np.int64)\n",
    "\n",
    "    if max_len is None:\n",
    "        # the LSTM input is only as long as the 95% longest\n",
    "        max_len = int(np.percentile(lengths_raw, 95))\n",
    "\n",
    "    N = len(tokenized_texts)\n",
    "    # here is the init of the \"embeddings\" matrix (initially full of <PAD> (0))\n",
    "    input_ids = np.full((N, max_len), fill_value=vocab[\"<PAD>\"], dtype=np.int64)\n",
    "    lengths = np.zeros(N, dtype=np.int64)\n",
    "\n",
    "    for i, toks in enumerate(tokenized_texts):\n",
    "        # THE embedding step\n",
    "        ids = [vocab.get(tok, vocab[\"<UNK>\"]) for tok in toks]\n",
    "        truncated = ids[:max_len]\n",
    "        input_ids[i, :len(truncated)] = truncated\n",
    "        lengths[i] = min(len(ids), max_len)\n",
    "\n",
    "    return input_ids, lengths, max_len\n",
    "\n",
    "\n",
    "\n",
    "# very simply use the bert tokenizer for bert. Assume same 95th percentile max length.\n",
    "# input_ids, same concept as above, but when fed into BERT, BERT internally maps the inputs\n",
    "def build_bert_features(texts, top95length):\n",
    "    from transformers import AutoTokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    encodings = tokenizer(\n",
    "        texts,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=top95length,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    return encodings[\"input_ids\"], encodings[\"attention_mask\"]\n",
    "\n",
    "\n",
    "\n",
    "# splits\n",
    "def split_indices(labels_main, test_size=0.25, val_size=0.25, seed=67):\n",
    "    \"\"\"\n",
    "    Returns train_idx, val_idx, test_idx (NumPy arrays of indices),\n",
    "    stratified by main labels.\n",
    "    \"\"\"\n",
    "    N = len(labels_main)\n",
    "    indices = np.arange(N)\n",
    "\n",
    "    train_idx, temp_idx = train_test_split(\n",
    "        indices,\n",
    "        test_size=test_size,\n",
    "        stratify=labels_main,\n",
    "        random_state=seed,\n",
    "    )\n",
    "\n",
    "    val_idx, test_idx = train_test_split(\n",
    "        temp_idx,\n",
    "        test_size=val_size,\n",
    "        stratify=labels_main[temp_idx],\n",
    "        random_state=seed,\n",
    "    )\n",
    "\n",
    "    return train_idx, val_idx, test_idx\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    # 1. Load raw data\n",
    "    texts, labels_main, labels_sub = load_wos_txt()\n",
    "    print(f\"Loaded {len(texts)} abstracts.\")\n",
    "\n",
    "    # 2. LSTM pipeline: tokenize -> vocab -> encode and pad\n",
    "    tokenized = tokenize_inputs(texts)\n",
    "    print(\"Example tokens:\", tokenized[0][:5])\n",
    "\n",
    "    vocab = build_vocab(tokenized, min_freq=3)\n",
    "    print(f\"LSTM vocab size (including PAD/UNK): {len(vocab)}\")\n",
    "\n",
    "    lstm_input_ids, lstm_lengths, max_len_lstm_used = encode_and_pad(\n",
    "        tokenized,\n",
    "        vocab\n",
    "    )\n",
    "    print(f\"LSTM encoded shape: {lstm_input_ids.shape} (max_len={max_len_lstm_used})\")\n",
    "\n",
    "    # 3. BERT pipeline: tokenizer -> input_ids + attention mask\n",
    "    bert_input_ids, bert_attention_mask = build_bert_features(\n",
    "        texts,\n",
    "        top95length=max_len_lstm_used\n",
    "    )\n",
    "    print(f\"BERT encoded shape: {bert_input_ids.shape} (max_len={max_len_lstm_used})\")\n",
    "\n",
    "    # 4. Shared train/val/test indices\n",
    "    train_idx, val_idx, test_idx = split_indices(\n",
    "        labels_main\n",
    "    )\n",
    "\n",
    "    # 5. Build LSTM splits\n",
    "    train_lstm = {\n",
    "        \"input_ids\": torch.from_numpy(lstm_input_ids[train_idx]),\n",
    "        \"lengths\": torch.from_numpy(lstm_lengths[train_idx]),\n",
    "        \"label_main\": torch.from_numpy(labels_main[train_idx]),\n",
    "        \"label_sub\": torch.from_numpy(labels_sub[train_idx]),\n",
    "    }\n",
    "    val_lstm = {\n",
    "        \"input_ids\": torch.from_numpy(lstm_input_ids[val_idx]),\n",
    "        \"lengths\": torch.from_numpy(lstm_lengths[val_idx]),\n",
    "        \"label_main\": torch.from_numpy(labels_main[val_idx]),\n",
    "        \"label_sub\": torch.from_numpy(labels_sub[val_idx]),\n",
    "    }\n",
    "    test_lstm = {\n",
    "        \"input_ids\": torch.from_numpy(lstm_input_ids[test_idx]),\n",
    "        \"lengths\": torch.from_numpy(lstm_lengths[test_idx]),\n",
    "        \"label_main\": torch.from_numpy(labels_main[test_idx]),\n",
    "        \"label_sub\": torch.from_numpy(labels_sub[test_idx]),\n",
    "    }\n",
    "\n",
    "    # 6. Build BERT splits\n",
    "    train_bert = {\n",
    "        \"input_ids\": bert_input_ids[train_idx],\n",
    "        \"attention_mask\": bert_attention_mask[train_idx],\n",
    "        \"label_main\": torch.from_numpy(labels_main[train_idx]),\n",
    "        \"label_sub\": torch.from_numpy(labels_sub[train_idx]),\n",
    "    }\n",
    "    val_bert = {\n",
    "        \"input_ids\": bert_input_ids[val_idx],\n",
    "        \"attention_mask\": bert_attention_mask[val_idx],\n",
    "        \"label_main\": torch.from_numpy(labels_main[val_idx]),\n",
    "        \"label_sub\": torch.from_numpy(labels_sub[val_idx]),\n",
    "    }\n",
    "    test_bert = {\n",
    "        \"input_ids\": bert_input_ids[test_idx],\n",
    "        \"attention_mask\": bert_attention_mask[test_idx],\n",
    "        \"label_main\": torch.from_numpy(labels_main[test_idx]),\n",
    "        \"label_sub\": torch.from_numpy(labels_sub[test_idx]),\n",
    "    }\n",
    "\n",
    "    # 7. Save everything\n",
    "    torch.save(train_lstm, \"wos_lstm_train.pt\")\n",
    "    torch.save(val_lstm, \"wos_lstm_val.pt\")\n",
    "    torch.save(test_lstm, \"wos_lstm_test.pt\")\n",
    "    print(\"Saved LSTM splits: wos_lstm_train.pt, wos_lstm_val.pt, wos_lstm_test.pt\")\n",
    "\n",
    "    torch.save(train_bert, \"wos_bert_train.pt\")\n",
    "    torch.save(val_bert, \"wos_bert_val.pt\")\n",
    "    torch.save(test_bert, \"wos_bert_test.pt\")\n",
    "    print(\"Saved BERT splits: wos_bert_train.pt, wos_bert_val.pt, wos_bert_test.pt\")\n",
    "\n",
    "    torch.save(\n",
    "        {\n",
    "            \"vocab\": vocab,\n",
    "            \"min_freq\": 3,\n",
    "            \"max_len_lstm\": max_len_lstm_used,\n",
    "        },\n",
    "        \"wos_vocab.pt\"\n",
    "    )\n",
    "    print(\"Saved vocab: wos_vocab.pt\")\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#useful functions\n",
    "def l2_loss(y, yh):\n",
    "  return 0.5 * (yh - y)**2\n",
    "\n",
    "def l2_loss_grad(y, yh):\n",
    "  return yh - y\n",
    "\n",
    "def cross_entropy(y, yh):\n",
    "  return -np.sum(y * np.log(yh + 1e-12))\n",
    "\n",
    "# note that this is true only for dL/dz, L = loss(softmax(z))\n",
    "def cross_entropy_grad(y, yh):\n",
    "  return yh - y\n",
    "\n",
    "def relu(x):\n",
    "  return np.maximum(0, x)\n",
    "   \n",
    "def relu_grad(x):\n",
    "    return (x > 0).astype(float)\n",
    "\n",
    "def leaky_relu(x, alpha=0.1):\n",
    "   return np.maximum(alpha*x, x)\n",
    "\n",
    "def leaky_relu_grad(x, alpha=0.1):\n",
    "    grad = np.ones_like(x)\n",
    "    grad[x < 0] = alpha\n",
    "    return grad\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_grad(x):\n",
    "    s = sigmoid(x)\n",
    "    return s * (1 - s)\n",
    "  \n",
    "def tanh(x):\n",
    "  return np.tanh(x)\n",
    "\n",
    "def tanh_grad(x):\n",
    "  t = np.tanh(x)\n",
    "  return 1 - t * t\n",
    "\n",
    "def linear(x):\n",
    "  return x\n",
    "\n",
    "def linear_grad(x):\n",
    "  return np.ones_like(x)\n",
    "\n",
    "def softmax(x):\n",
    "  z = x - np.max(x)\n",
    "  e = np.exp(z)\n",
    "  return e / np.sum(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#task 2 implement LSTM\n",
    "\n",
    "class Cell:\n",
    "    def __init__(self, input_size, hidden_size, b_f=None, b_i=None, b_o=None, b_c=None):\n",
    "        #random weight matrices for now\n",
    "        self.W_f = np.random.randn(hidden_size, input_size + hidden_size) * 0.01\n",
    "        self.W_i = np.random.randn(hidden_size, input_size + hidden_size) * 0.01\n",
    "        self.W_c = np.random.randn(hidden_size, input_size + hidden_size) * 0.01\n",
    "        self.W_o = np.random.randn(hidden_size, input_size + hidden_size) * 0.01\n",
    "        #initialize biases\n",
    "        self.b_f = b_f if b_f is not None else np.ones(hidden_size)  \n",
    "        self.b_i = b_i if b_i is not None else np.zeros(hidden_size)\n",
    "        self.b_o = b_o if b_o is not None else np.zeros(hidden_size)\n",
    "        self.b_c = b_c if b_c is not None else np.zeros(hidden_size)\n",
    "\n",
    "    #forget gate\n",
    "    def get_f_t(self, h_prev, x): \n",
    "        \"\"\"\"\n",
    "        computes the forget value given h_{t-1} (prev hidden state), x_t (input), b_f (current forget bias)\n",
    "    \n",
    "        \"\"\"\n",
    "        concat = np.concatenate([h_prev, x], axis=-1)\n",
    "        return sigmoid(np.dot(self.W_f, concat) + self.b_f)\n",
    "    \n",
    "    #input gate\n",
    "    def get_i_t(self, h_prev, x):\n",
    "        concat = np.concatenate([h_prev, x], axis=-1)\n",
    "        return sigmoid(np.dot(self.W_i, concat) + self.b_i)\n",
    "    \n",
    "    def get_c_hat(self, h_prev, x):\n",
    "        concat = np.concatenate([h_prev, x], axis=-1)\n",
    "        return tanh(np.dot(self.W_c, concat) + self.b_c)\n",
    "    \n",
    "    def get_c_t(self, c_prev, h_prev, x):\n",
    "        c_hat = self.get_c_hat(h_prev, x)\n",
    "        i_t = self.get_i_t(h_prev, x)\n",
    "        forget = self.get_f_t(h_prev, x)\n",
    "        return forget * c_prev + i_t * c_hat\n",
    "    \n",
    "    #output gate\n",
    "    def get_o_t(self, h_prev, x):\n",
    "        concat = np.concatenate([h_prev, x], axis=-1)\n",
    "        return sigmoid(np.dot(self.W_o, concat) + self.b_o)\n",
    "\n",
    "    def get_h(self, o_t, c_t):\n",
    "        return o_t * tanh(c_t)\n",
    "    \n",
    "    def forward(self, h_prev, c_prev, x): # <<<<<<<<<<<<<<<---------------------------- use this!\n",
    "        \"\"\" \n",
    "        computes a forward pass of the entire cell\n",
    "        returns: h_t, c_t\n",
    "        \"\"\"\n",
    "        f_t = self.get_f_t(h_prev, x)\n",
    "        i_t = self.get_i_t(h_prev, x)\n",
    "        c_t = self.get_c_t(c_prev, h_prev, x)\n",
    "        o_t = self.get_o_t(h_prev, x)\n",
    "        h_t = self.get_h(o_t, c_t)\n",
    "        self.cache = {\n",
    "            'h_prev': h_prev, 'c_prev': c_prev, 'x': x,\n",
    "            'f_t': f_t, 'i_t': i_t, 'c_hat': c_hat,\n",
    "            'c_t': c_t, 'o_t': o_t, 'concat': concat\n",
    "        }\n",
    "        return h_t, c_t\n",
    "    \n",
    "    def backward(self, dh_next, dc_next):\n",
    "        \"\"\"\n",
    "        dh_next: gradient of loss w.r.t h_t (from output or next timestep)\n",
    "        dc_next: gradient of loss w.r.t C_t (from next timestep)\n",
    "        returns: dx, dh_prev, dc_prev\n",
    "        and stores dW/db for optimizer\n",
    "        \"\"\"\n",
    "        # retrieve cached values\n",
    "        f_t = self.cache['f_t']\n",
    "        i_t = self.cache['i_t']\n",
    "        c_hat = self.cache['c_hat']\n",
    "        o_t = self.cache['o_t']\n",
    "        c_prev = self.cache['c_prev']\n",
    "        concat = self.cache['concat']\n",
    "\n",
    "        c_t = self.cache['c_t']\n",
    "\n",
    "        # derivative of loss w.r.t c_t (total)\n",
    "        dc_t = dh_next * o_t * (1 - np.tanh(c_t)**2) + dc_next\n",
    "\n",
    "        # derivatives w.r.t gates before activation\n",
    "        do = dh_next * np.tanh(c_t)\n",
    "        di = dc_t * c_hat\n",
    "        df = dc_t * c_prev\n",
    "        dc_hat = dc_t * i_t\n",
    "\n",
    "        # apply activation derivatives\n",
    "        dZ_o = do * o_t * (1 - o_t)\n",
    "        dZ_i = di * i_t * (1 - i_t)\n",
    "        dZ_f = df * f_t * (1 - f_t)\n",
    "        dZ_c = dc_hat * (1 - c_hat**2)\n",
    "\n",
    "        # gradients w.r.t weights and biases\n",
    "        self.dW_o = np.outer(dZ_o, concat)\n",
    "        self.dW_i = np.outer(dZ_i, concat)\n",
    "        self.dW_f = np.outer(dZ_f, concat)\n",
    "        self.dW_c = np.outer(dZ_c, concat)\n",
    "\n",
    "        self.db_o = dZ_o\n",
    "        self.db_i = dZ_i\n",
    "        self.db_f = dZ_f\n",
    "        self.db_c = dZ_c\n",
    "\n",
    "        # gradient w.r.t concat\n",
    "        dconcat = (self.W_o.T @ dZ_o +\n",
    "                   self.W_i.T @ dZ_i +\n",
    "                   self.W_f.T @ dZ_f +\n",
    "                   self.W_c.T @ dZ_c)\n",
    "\n",
    "        # split into dx and dh_prev\n",
    "        hidden_size = self.cache['h_prev'].shape[0]\n",
    "        dx = dconcat[hidden_size:]\n",
    "        dh_prev = dconcat[:hidden_size]\n",
    "        dc_prev = dc_t * f_t\n",
    "\n",
    "        return dx, dh_prev, dc_prev\n",
    "\n",
    "\n",
    "class LSTM: # note: I hardcoded seq2vec based on the assignment description\n",
    "\n",
    "    def __init__(self, input_size, hidden_size): #initialize a network with #cell_count cells\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.cell = Cell(input_size, hidden_size)\n",
    "        self.h_cache = []   # stores all h_t\n",
    "        self.c_cache = []   # stores all c_t\n",
    "        self.gates_cache = []  # optional, stores f_t, i_t, o_t, c_hat\n",
    "\n",
    "    def fit(self, X_seqs, Y_vecs, batch_size=1, lr=0.01, epochs=10):\n",
    "        \"\"\"\n",
    "        X_seqs: list of sequences, each sequence is array of shape (seq_len, input_size)\n",
    "        Y_vecs: list of target vectors, each of shape (output_size,)\n",
    "        batch_size: number of sequences per batch (can start with 1)\n",
    "        lr: learning rate\n",
    "        epochs: number of training passes\n",
    "        \"\"\"\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "\n",
    "            # optionally shuffle data\n",
    "            for i in range(0, len(X_seqs), batch_size):\n",
    "                batch_X = X_seqs[i:i+batch_size]\n",
    "                batch_Y = Y_vecs[i:i+batch_size]\n",
    "\n",
    "                # initialize gradient accumulators\n",
    "                self.cell.reset_gradients()  #\n",
    "\n",
    "                for x_seq, y_vec in zip(batch_X, batch_Y):\n",
    "                    # 1️ Forward pass through the sequence\n",
    "                    h_prev = np.zeros(self.hidden_size)\n",
    "                    c_prev = np.zeros(self.hidden_size)\n",
    "\n",
    "                    for x_t in x_seq:\n",
    "                        h_prev, c_prev = self.cell.forward(h_prev, c_prev, x_t)\n",
    "\n",
    "                    # h_prev is now the final hidden state -> seq2vec\n",
    "                    y_pred = h_prev  # or pass through a linear layer if you want\n",
    "\n",
    "                    # 2️ Compute loss (MSE example)\n",
    "                    loss = 0.5 * np.sum((y_pred - y_vec)**2)\n",
    "                    total_loss += loss\n",
    "\n",
    "                    # 3️ Backward pass (BPTT)\n",
    "                    dh = y_pred - y_vec\n",
    "                    dc = np.zeros_like(c_prev)\n",
    "\n",
    "                    for t in reversed(range(len(x_seq))):\n",
    "                        # call cell.backward() with dh and dc\n",
    "                        dx, dh, dc = self.cell.backward(dh, dc)\n",
    "                        # cell.backward accumulates gradients internally\n",
    "\n",
    "                # 4️ Update weights after batch\n",
    "                self.cell.W_f -= lr * self.cell.dW_f\n",
    "                self.cell.b_f -= lr * self.cell.db_f\n",
    "                self.cell.W_i -= lr * self.cell.dW_i\n",
    "                self.cell.b_i -= lr * self.cell.db_i\n",
    "                self.cell.W_c -= lr * self.cell.dW_c\n",
    "                self.cell.b_c -= lr * self.cell.db_c\n",
    "                self.cell.W_o -= lr * self.cell.dW_o\n",
    "                self.cell.b_o -= lr * self.cell.db_o\n",
    "\n",
    "            print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")\n",
    "\n",
    "    def predict(self, X_seq):\n",
    "        h_prev = np.zeros(self.hidden_size)\n",
    "        c_prev = np.zeros(self.hidden_size)\n",
    "\n",
    "        for x_t in X_seq:\n",
    "            h_prev, c_prev = self.cell.forward(h_prev, c_prev, x_t)\n",
    "\n",
    "        y_pred = h_prev\n",
    "        return y_pred\n",
    "    \n",
    "    def evaluate_acc(self, X_seq, Y_vec):\n",
    "        y_pred = self.predict(X_seq)\n",
    "        correct=0\n",
    "        for i in range(len(y_pred)):\n",
    "            if y_pred[i] == Y_vec[i]:\n",
    "                correct+=1\n",
    "        return correct/len(Y_vec)\n",
    "\n",
    "\n",
    "#TODO implement testing\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 completed.\n",
      "Epoch 2/3 completed.\n",
      "Epoch 3/3 completed.\n",
      "Predicted classes: tensor([4, 1, 1])\n",
      "True labels: tensor([0, 2, 1])\n"
     ]
    }
   ],
   "source": [
    "# TASK 2 part b: BERT model\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim import AdamW\n",
    "\n",
    "# Example dataset (replace with your WOS data)\n",
    "texts = [\n",
    "    \"This is an example abstract 1.\",\n",
    "    \"Another abstract for testing.\",\n",
    "    \"Yet another example abstract.\"\n",
    "]\n",
    "labels = [0, 2, 1]  # integer class labels for each abstract\n",
    "\n",
    "# Parameters\n",
    "num_labels = 7          # number of top-level classes in WOS\n",
    "max_len = 128           # max token length\n",
    "batch_size = 2\n",
    "epochs = 3\n",
    "lr = 2e-5\n",
    "\n",
    "# Load pre-trained tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=num_labels)\n",
    "\n",
    "# Tokenize all texts\n",
    "encodings = tokenizer(texts, padding=True, truncation=True, max_length=max_len, return_tensors=\"pt\")\n",
    "input_ids = encodings[\"input_ids\"]\n",
    "attention_mask = encodings[\"attention_mask\"]\n",
    "labels_tensor = torch.tensor(labels)\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = TensorDataset(input_ids, attention_mask, labels_tensor)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    for batch in dataloader:\n",
    "        b_input_ids, b_attention_mask, b_labels = batch\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids=b_input_ids, attention_mask=b_attention_mask, labels=b_labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}/{epochs} completed.\")\n",
    "\n",
    "# Evaluation / Prediction\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    logits = outputs.logits\n",
    "    preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "print(\"Predicted classes:\", preds)\n",
    "print(\"True labels:\", labels_tensor)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
