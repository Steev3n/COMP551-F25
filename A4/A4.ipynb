{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/bin/python3\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#task 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#useful functions\n",
    "def l2_loss(y, yh):\n",
    "  return 0.5 * (yh - y)**2\n",
    "\n",
    "def l2_loss_grad(y, yh):\n",
    "  return yh - y\n",
    "\n",
    "def cross_entropy(y, yh):\n",
    "  return -np.sum(y * np.log(yh + 1e-12))\n",
    "\n",
    "# note that this is true only for dL/dz, L = loss(softmax(z))\n",
    "def cross_entropy_grad(y, yh):\n",
    "  return yh - y\n",
    "\n",
    "def relu(x):\n",
    "  return np.maximum(0, x)\n",
    "   \n",
    "def relu_grad(x):\n",
    "    return (x > 0).astype(float)\n",
    "\n",
    "def leaky_relu(x, alpha=0.1):\n",
    "   return np.maximum(alpha*x, x)\n",
    "\n",
    "def leaky_relu_grad(x, alpha=0.1):\n",
    "    grad = np.ones_like(x)\n",
    "    grad[x < 0] = alpha\n",
    "    return grad\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_grad(x):\n",
    "    s = sigmoid(x)\n",
    "    return s * (1 - s)\n",
    "  \n",
    "def tanh(x):\n",
    "  return np.tanh(x)\n",
    "\n",
    "def tanh_grad(x):\n",
    "  t = np.tanh(x)\n",
    "  return 1 - t * t\n",
    "\n",
    "def linear(x):\n",
    "  return x\n",
    "\n",
    "def linear_grad(x):\n",
    "  return np.ones_like(x)\n",
    "\n",
    "def softmax(x):\n",
    "  z = x - np.max(x)\n",
    "  e = np.exp(z)\n",
    "  return e / np.sum(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#task 2 implement LSTM\n",
    "\n",
    "class Cell:\n",
    "    def __init__(self, input_size, hidden_size, b_f=None, b_i=None, b_o=None, b_c=None):\n",
    "        #random weight matrices for now\n",
    "        self.W_f = np.random.randn(hidden_size, input_size + hidden_size) * 0.01\n",
    "        self.W_i = np.random.randn(hidden_size, input_size + hidden_size) * 0.01\n",
    "        self.W_c = np.random.randn(hidden_size, input_size + hidden_size) * 0.01\n",
    "        self.W_o = np.random.randn(hidden_size, input_size + hidden_size) * 0.01\n",
    "        #initialize biases\n",
    "        self.b_f = b_f if b_f is not None else np.ones(hidden_size)  \n",
    "        self.b_i = b_i if b_i is not None else np.zeros(hidden_size)\n",
    "        self.b_o = b_o if b_o is not None else np.zeros(hidden_size)\n",
    "        self.b_c = b_c if b_c is not None else np.zeros(hidden_size)\n",
    "\n",
    "    #forget gate\n",
    "    def get_f_t(self, h_prev, x): \n",
    "        \"\"\"\"\n",
    "        computes the forget value given h_{t-1} (prev hidden state), x_t (input), b_f (current forget bias)\n",
    "    \n",
    "        \"\"\"\n",
    "        concat = np.concatenate([h_prev, x], axis=-1)\n",
    "        return sigmoid(np.dot(self.W_f, concat) + self.b_f)\n",
    "    \n",
    "    #input gate\n",
    "    def get_i_t(self, h_prev, x):\n",
    "        concat = np.concatenate([h_prev, x], axis=-1)\n",
    "        return sigmoid(np.dot(self.W_i, concat) + self.b_i)\n",
    "    \n",
    "    def get_c_hat(self, h_prev, x):\n",
    "        concat = np.concatenate([h_prev, x], axis=-1)\n",
    "        return tanh(np.dot(self.W_c, concat) + self.b_c)\n",
    "    \n",
    "    def get_c_t(self, c_prev, h_prev, x):\n",
    "        c_hat = self.get_c_hat(h_prev, x)\n",
    "        i_t = self.get_i_t(h_prev, x)\n",
    "        forget = self.get_f_t(h_prev, x)\n",
    "        return forget * c_prev + i_t * c_hat\n",
    "    \n",
    "    #output gate\n",
    "    def get_o_t(self, h_prev, x):\n",
    "        concat = np.concatenate([h_prev, x], axis=-1)\n",
    "        return sigmoid(np.dot(self.W_o, concat) + self.b_o)\n",
    "\n",
    "    def get_h(self, o_t, c_t):\n",
    "        return o_t * tanh(c_t)\n",
    "    \n",
    "    def forward(self, h_prev, c_prev, x): # <<<<<<<<<<<<<<<---------------------------- use this!\n",
    "        \"\"\" \n",
    "        computes a forward pass of the entire cell\n",
    "        returns: h_t, c_t\n",
    "        \"\"\"\n",
    "        f_t = self.get_f_t(h_prev, x)\n",
    "        i_t = self.get_i_t(h_prev, x)\n",
    "        c_t = self.get_c_t(c_prev, h_prev, x)\n",
    "        o_t = self.get_o_t(h_prev, x)\n",
    "        h_t = self.get_h(o_t, c_t)\n",
    "        self.cache = {\n",
    "            'h_prev': h_prev, 'c_prev': c_prev, 'x': x,\n",
    "            'f_t': f_t, 'i_t': i_t, 'c_hat': c_hat,\n",
    "            'c_t': c_t, 'o_t': o_t, 'concat': concat\n",
    "        }\n",
    "        return h_t, c_t\n",
    "    \n",
    "    def backward(self, dh_next, dc_next):\n",
    "        \"\"\"\n",
    "        dh_next: gradient of loss w.r.t h_t (from output or next timestep)\n",
    "        dc_next: gradient of loss w.r.t C_t (from next timestep)\n",
    "        returns: dx, dh_prev, dc_prev\n",
    "        and stores dW/db for optimizer\n",
    "        \"\"\"\n",
    "        # retrieve cached values\n",
    "        f_t = self.cache['f_t']\n",
    "        i_t = self.cache['i_t']\n",
    "        c_hat = self.cache['c_hat']\n",
    "        o_t = self.cache['o_t']\n",
    "        c_prev = self.cache['c_prev']\n",
    "        concat = self.cache['concat']\n",
    "\n",
    "        c_t = self.cache['c_t']\n",
    "\n",
    "        # derivative of loss w.r.t c_t (total)\n",
    "        dc_t = dh_next * o_t * (1 - np.tanh(c_t)**2) + dc_next\n",
    "\n",
    "        # derivatives w.r.t gates before activation\n",
    "        do = dh_next * np.tanh(c_t)\n",
    "        di = dc_t * c_hat\n",
    "        df = dc_t * c_prev\n",
    "        dc_hat = dc_t * i_t\n",
    "\n",
    "        # apply activation derivatives\n",
    "        dZ_o = do * o_t * (1 - o_t)\n",
    "        dZ_i = di * i_t * (1 - i_t)\n",
    "        dZ_f = df * f_t * (1 - f_t)\n",
    "        dZ_c = dc_hat * (1 - c_hat**2)\n",
    "\n",
    "        # gradients w.r.t weights and biases\n",
    "        self.dW_o = np.outer(dZ_o, concat)\n",
    "        self.dW_i = np.outer(dZ_i, concat)\n",
    "        self.dW_f = np.outer(dZ_f, concat)\n",
    "        self.dW_c = np.outer(dZ_c, concat)\n",
    "\n",
    "        self.db_o = dZ_o\n",
    "        self.db_i = dZ_i\n",
    "        self.db_f = dZ_f\n",
    "        self.db_c = dZ_c\n",
    "\n",
    "        # gradient w.r.t concat\n",
    "        dconcat = (self.W_o.T @ dZ_o +\n",
    "                   self.W_i.T @ dZ_i +\n",
    "                   self.W_f.T @ dZ_f +\n",
    "                   self.W_c.T @ dZ_c)\n",
    "\n",
    "        # split into dx and dh_prev\n",
    "        hidden_size = self.cache['h_prev'].shape[0]\n",
    "        dx = dconcat[hidden_size:]\n",
    "        dh_prev = dconcat[:hidden_size]\n",
    "        dc_prev = dc_t * f_t\n",
    "\n",
    "        return dx, dh_prev, dc_prev\n",
    "\n",
    "\n",
    "class LSTM: # note: I hardcoded seq2vec based on the assignment description\n",
    "\n",
    "    def __init__(self, input_size, hidden_size): #initialize a network with #cell_count cells\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.cell = Cell(input_size, hidden_size)\n",
    "        self.h_cache = []   # stores all h_t\n",
    "        self.c_cache = []   # stores all c_t\n",
    "        self.gates_cache = []  # optional, stores f_t, i_t, o_t, c_hat\n",
    "\n",
    "    def fit(self, X_seqs, Y_vecs, batch_size=1, lr=0.01, epochs=10):\n",
    "        \"\"\"\n",
    "        X_seqs: list of sequences, each sequence is array of shape (seq_len, input_size)\n",
    "        Y_vecs: list of target vectors, each of shape (output_size,)\n",
    "        batch_size: number of sequences per batch (can start with 1)\n",
    "        lr: learning rate\n",
    "        epochs: number of training passes\n",
    "        \"\"\"\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "\n",
    "            # optionally shuffle data\n",
    "            for i in range(0, len(X_seqs), batch_size):\n",
    "                batch_X = X_seqs[i:i+batch_size]\n",
    "                batch_Y = Y_vecs[i:i+batch_size]\n",
    "\n",
    "                # initialize gradient accumulators\n",
    "                self.cell.reset_gradients()  #\n",
    "\n",
    "                for x_seq, y_vec in zip(batch_X, batch_Y):\n",
    "                    # 1️ Forward pass through the sequence\n",
    "                    h_prev = np.zeros(self.hidden_size)\n",
    "                    c_prev = np.zeros(self.hidden_size)\n",
    "\n",
    "                    for x_t in x_seq:\n",
    "                        h_prev, c_prev = self.cell.forward(h_prev, c_prev, x_t)\n",
    "\n",
    "                    # h_prev is now the final hidden state -> seq2vec\n",
    "                    y_pred = h_prev  # or pass through a linear layer if you want\n",
    "\n",
    "                    # 2️ Compute loss (MSE example)\n",
    "                    loss = 0.5 * np.sum((y_pred - y_vec)**2)\n",
    "                    total_loss += loss\n",
    "\n",
    "                    # 3️ Backward pass (BPTT)\n",
    "                    dh = y_pred - y_vec\n",
    "                    dc = np.zeros_like(c_prev)\n",
    "\n",
    "                    for t in reversed(range(len(x_seq))):\n",
    "                        # call cell.backward() with dh and dc\n",
    "                        dx, dh, dc = self.cell.backward(dh, dc)\n",
    "                        # cell.backward accumulates gradients internally\n",
    "\n",
    "                # 4️ Update weights after batch\n",
    "                self.cell.W_f -= lr * self.cell.dW_f\n",
    "                self.cell.b_f -= lr * self.cell.db_f\n",
    "                self.cell.W_i -= lr * self.cell.dW_i\n",
    "                self.cell.b_i -= lr * self.cell.db_i\n",
    "                self.cell.W_c -= lr * self.cell.dW_c\n",
    "                self.cell.b_c -= lr * self.cell.db_c\n",
    "                self.cell.W_o -= lr * self.cell.dW_o\n",
    "                self.cell.b_o -= lr * self.cell.db_o\n",
    "\n",
    "            print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")\n",
    "\n",
    "    def predict(self, X_seq):\n",
    "        h_prev = np.zeros(self.hidden_size)\n",
    "        c_prev = np.zeros(self.hidden_size)\n",
    "\n",
    "        for x_t in X_seq:\n",
    "            h_prev, c_prev = self.cell.forward(h_prev, c_prev, x_t)\n",
    "\n",
    "        y_pred = h_prev\n",
    "        return y_pred\n",
    "    \n",
    "    def evaluate_acc(self, X_seq, Y_vec):\n",
    "        y_pred = self.predict(X_seq)\n",
    "        correct=0\n",
    "        for i in range(len(y_pred)):\n",
    "            if y_pred[i] == Y_vec[i]:\n",
    "                correct+=1\n",
    "        return correct/len(Y_vec)\n",
    "\n",
    "\n",
    "#TODO implement testing\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 completed.\n",
      "Epoch 2/3 completed.\n",
      "Epoch 3/3 completed.\n",
      "Predicted classes: tensor([4, 1, 1])\n",
      "True labels: tensor([0, 2, 1])\n"
     ]
    }
   ],
   "source": [
    "# TASK 2 part b: BERT model\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim import AdamW\n",
    "\n",
    "# Example dataset (replace with your WOS data)\n",
    "texts = [\n",
    "    \"This is an example abstract 1.\",\n",
    "    \"Another abstract for testing.\",\n",
    "    \"Yet another example abstract.\"\n",
    "]\n",
    "labels = [0, 2, 1]  # integer class labels for each abstract\n",
    "\n",
    "# Parameters\n",
    "num_labels = 7          # number of top-level classes in WOS\n",
    "max_len = 128           # max token length\n",
    "batch_size = 2\n",
    "epochs = 3\n",
    "lr = 2e-5\n",
    "\n",
    "# Load pre-trained tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=num_labels)\n",
    "\n",
    "# Tokenize all texts\n",
    "encodings = tokenizer(texts, padding=True, truncation=True, max_length=max_len, return_tensors=\"pt\")\n",
    "input_ids = encodings[\"input_ids\"]\n",
    "attention_mask = encodings[\"attention_mask\"]\n",
    "labels_tensor = torch.tensor(labels)\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = TensorDataset(input_ids, attention_mask, labels_tensor)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    for batch in dataloader:\n",
    "        b_input_ids, b_attention_mask, b_labels = batch\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids=b_input_ids, attention_mask=b_attention_mask, labels=b_labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}/{epochs} completed.\")\n",
    "\n",
    "# Evaluation / Prediction\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    logits = outputs.logits\n",
    "    preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "print(\"Predicted classes:\", preds)\n",
    "print(\"True labels:\", labels_tensor)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
