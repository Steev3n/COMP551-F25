{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/bin/python3\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#task 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#useful functions\n",
    "def l2_loss(y, yh):\n",
    "  return 0.5 * (yh - y)**2\n",
    "\n",
    "def l2_loss_grad(y, yh):\n",
    "  return yh - y\n",
    "\n",
    "def cross_entropy(y, yh):\n",
    "  return -np.sum(y * np.log(yh + 1e-12))\n",
    "\n",
    "# note that this is true only for dL/dz, L = loss(softmax(z))\n",
    "def cross_entropy_grad(y, yh):\n",
    "  return yh - y\n",
    "\n",
    "def relu(x):\n",
    "  return np.maximum(0, x)\n",
    "   \n",
    "def relu_grad(x):\n",
    "    return (x > 0).astype(float)\n",
    "\n",
    "def leaky_relu(x, alpha=0.1):\n",
    "   return np.maximum(alpha*x, x)\n",
    "\n",
    "def leaky_relu_grad(x, alpha=0.1):\n",
    "    grad = np.ones_like(x)\n",
    "    grad[x < 0] = alpha\n",
    "    return grad\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_grad(x):\n",
    "    s = sigmoid(x)\n",
    "    return s * (1 - s)\n",
    "  \n",
    "def tanh(x):\n",
    "  return np.tanh(x)\n",
    "\n",
    "def tanh_grad(x):\n",
    "  t = np.tanh(x)\n",
    "  return 1 - t * t\n",
    "\n",
    "def linear(x):\n",
    "  return x\n",
    "\n",
    "def linear_grad(x):\n",
    "  return np.ones_like(x)\n",
    "\n",
    "def softmax(x):\n",
    "  z = x - np.max(x)\n",
    "  e = np.exp(z)\n",
    "  return e / np.sum(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#task 2 implement LSTM\n",
    "\n",
    "class Cell:\n",
    "    def __init__(self, input_size, hidden_size, b_f=None, b_i=None, b_o=None, b_c=None):\n",
    "        #random weight matrices for now\n",
    "        self.W_f = np.random.randn(hidden_size, input_size + hidden_size) * 0.01\n",
    "        self.W_i = np.random.randn(hidden_size, input_size + hidden_size) * 0.01\n",
    "        self.W_c = np.random.randn(hidden_size, input_size + hidden_size) * 0.01\n",
    "        self.W_o = np.random.randn(hidden_size, input_size + hidden_size) * 0.01\n",
    "        #initialize biases\n",
    "        self.b_f = b_f if b_f is not None else np.ones(hidden_size)  \n",
    "        self.b_i = b_i if b_i is not None else np.zeros(hidden_size)\n",
    "        self.b_o = b_o if b_o is not None else np.zeros(hidden_size)\n",
    "        self.b_c = b_c if b_c is not None else np.zeros(hidden_size)\n",
    "\n",
    "    #forget gate\n",
    "    def get_f_t(self, h_prev, x): \n",
    "        \"\"\"\"\n",
    "        computes the forget value given h_{t-1} (prev hidden state), x_t (input), b_f (current forget bias)\n",
    "    \n",
    "        \"\"\"\n",
    "        concat = np.concatenate([h_prev, x], axis=-1)\n",
    "        return sigmoid(np.dot(self.W_f, concat) + self.b_f)\n",
    "    \n",
    "    #input gate\n",
    "    def get_i_t(self, h_prev, x):\n",
    "        concat = np.concatenate([h_prev, x], axis=-1)\n",
    "        return sigmoid(np.dot(self.W_i, concat) + self.b_i)\n",
    "    \n",
    "    def get_c_hat(self, h_prev, x):\n",
    "        concat = np.concatenate([h_prev, x], axis=-1)\n",
    "        return tanh(np.dot(self.W_c, concat) + self.b_c)\n",
    "    \n",
    "    def get_c_t(self, c_prev, h_prev, x):\n",
    "        c_hat = self.get_c_hat(h_prev, x)\n",
    "        i_t = self.get_i_t(h_prev, x)\n",
    "        forget = self.get_f_t(h_prev, x)\n",
    "        return forget * c_prev + i_t * c_hat\n",
    "    \n",
    "    #output gate\n",
    "    def get_o_t(self, h_prev, x):\n",
    "        concat = np.concatenate([h_prev, x], axis=-1)\n",
    "        return sigmoid(np.dot(self.W_o, concat) + self.b_o)\n",
    "\n",
    "    def get_h(self, o_t, c_t):\n",
    "        return o_t * tanh(c_t)\n",
    "    \n",
    "    def forward(self, h_prev, c_prev, x): # <<<<<<<<<<<<<<<---------------------------- use this!\n",
    "        \"\"\" \n",
    "        computes a forward pass of the entire cell\n",
    "        returns: h_t, c_t\n",
    "        \"\"\"\n",
    "        f_t = self.get_f_t(h_prev, x)\n",
    "        i_t = self.get_i_t(h_prev, x)\n",
    "        c_t = self.get_c_t(c_prev, h_prev, x)\n",
    "        o_t = self.get_o_t(h_prev, x)\n",
    "        h_t = self.get_h(o_t, c_t)\n",
    "        self.cache = {\n",
    "            'h_prev': h_prev, 'c_prev': c_prev, 'x': x,\n",
    "            'f_t': f_t, 'i_t': i_t, 'c_hat': c_hat,\n",
    "            'c_t': c_t, 'o_t': o_t, 'concat': concat\n",
    "        }\n",
    "        return h_t, c_t\n",
    "    \n",
    "    def backward(self, dh_next, dc_next):\n",
    "        \"\"\"\n",
    "        dh_next: gradient of loss w.r.t h_t (from output or next timestep)\n",
    "        dc_next: gradient of loss w.r.t C_t (from next timestep)\n",
    "        returns: dx, dh_prev, dc_prev\n",
    "        and stores dW/db for optimizer\n",
    "        \"\"\"\n",
    "        # retrieve cached values\n",
    "        f_t = self.cache['f_t']\n",
    "        i_t = self.cache['i_t']\n",
    "        c_hat = self.cache['c_hat']\n",
    "        o_t = self.cache['o_t']\n",
    "        c_prev = self.cache['c_prev']\n",
    "        concat = self.cache['concat']\n",
    "\n",
    "        c_t = self.cache['c_t']\n",
    "\n",
    "        # derivative of loss w.r.t c_t (total)\n",
    "        dc_t = dh_next * o_t * (1 - np.tanh(c_t)**2) + dc_next\n",
    "\n",
    "        # derivatives w.r.t gates before activation\n",
    "        do = dh_next * np.tanh(c_t)\n",
    "        di = dc_t * c_hat\n",
    "        df = dc_t * c_prev\n",
    "        dc_hat = dc_t * i_t\n",
    "\n",
    "        # apply activation derivatives\n",
    "        dZ_o = do * o_t * (1 - o_t)\n",
    "        dZ_i = di * i_t * (1 - i_t)\n",
    "        dZ_f = df * f_t * (1 - f_t)\n",
    "        dZ_c = dc_hat * (1 - c_hat**2)\n",
    "\n",
    "        # gradients w.r.t weights and biases\n",
    "        self.dW_o = np.outer(dZ_o, concat)\n",
    "        self.dW_i = np.outer(dZ_i, concat)\n",
    "        self.dW_f = np.outer(dZ_f, concat)\n",
    "        self.dW_c = np.outer(dZ_c, concat)\n",
    "\n",
    "        self.db_o = dZ_o\n",
    "        self.db_i = dZ_i\n",
    "        self.db_f = dZ_f\n",
    "        self.db_c = dZ_c\n",
    "\n",
    "        # gradient w.r.t concat\n",
    "        dconcat = (self.W_o.T @ dZ_o +\n",
    "                   self.W_i.T @ dZ_i +\n",
    "                   self.W_f.T @ dZ_f +\n",
    "                   self.W_c.T @ dZ_c)\n",
    "\n",
    "        # split into dx and dh_prev\n",
    "        hidden_size = self.cache['h_prev'].shape[0]\n",
    "        dx = dconcat[hidden_size:]\n",
    "        dh_prev = dconcat[:hidden_size]\n",
    "        dc_prev = dc_t * f_t\n",
    "\n",
    "        return dx, dh_prev, dc_prev\n",
    "\n",
    "\n",
    "class LSTM:\n",
    "\n",
    "    def __init__(self, cell_count): #initialize a network with #cell_count cells\n",
    "        for i in range(cell_count)\n",
    "    def fit(self, X, Y, batch_size, lr, epochs):\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
